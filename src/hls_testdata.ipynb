{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO:\\n- MSEloss for all layer tests\\n- Proper data generation method (multiple datasets needed to test everything)\\n- Test for ints and big float as well\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "- MSEloss for all layer tests\n",
    "- Proper data generation method (multiple datasets needed to test everything)\n",
    "- Test for ints and big float as well\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from I2DL exercises, need to cite!\n",
    "\n",
    "# def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "#     \"\"\"\n",
    "#     Evaluate a numeric gradient for a function that accepts a numpy\n",
    "#     array and returns a numpy array.\n",
    "#     \"\"\"\n",
    "#     grad = np.zeros_like(x)\n",
    "#     it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "#     while not it.finished:\n",
    "#         ix = it.multi_index\n",
    "\n",
    "#         oldval = x[ix]\n",
    "#         x[ix] = oldval + h\n",
    "#         pos = f(x).copy()\n",
    "#         x[ix] = oldval - h\n",
    "#         neg = f(x).copy()\n",
    "#         x[ix] = oldval\n",
    "\n",
    "#         grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "#         it.iternext()\n",
    "#     return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def save_var(path, name, npy_arrays: Dict[str, Any], it):\n",
    "    \"\"\"\n",
    "    Helper function to save Numpy arrays to use for testing HLS components.\n",
    "    path: location of folder where files are saved to\n",
    "    name: appended to file name of every Numpy file\n",
    "    npy_arrays: Dict of numpy arrays to save, key is used to name file, value is the Numpy array, not PyTorch tensor!.\n",
    "    it: iteration number, appended to end of file name\n",
    "    \"\"\"\n",
    "    for key, val in npy_arrays.items():\n",
    "        with open(path / f\"{name}_{key}_{it}.npy\", \"wb\") as f:\n",
    "            np.save(f, val)\n",
    "\n",
    "        print(f\"Saved variable {key} to {path}\")\n",
    "\n",
    "# See https://numpy.org/doc/stable/reference/generated/numpy.pad.html\n",
    "# Only pads the Channels, does not add batches\n",
    "def pad_with(tensor, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 10)\n",
    "    # print(pad_width)\n",
    "    tensor[:pad_width[0]] = pad_value\n",
    "    tensor[-pad_width[1]:] = pad_value\n",
    "\n",
    "# def pad_with(tensor, pad_width, iaxis, kwargs):\n",
    "#     pad_value = kwargs.get('padder', 10)\n",
    "#     vector[:pad_width[0]] = pad_value\n",
    "#     vector[-pad_width[1]:] = pad_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x gradients are approximately the same: True\n",
      "Saved variable x to ../../mtt_components/relu/tests\n",
      "Saved variable dx to ../../mtt_components/relu/tests\n",
      "Saved variable out to ../../mtt_components/relu/tests\n",
      "Saved variable dout to ../../mtt_components/relu/tests\n"
     ]
    }
   ],
   "source": [
    "# ReLU testdata generation\n",
    "\n",
    "# First val is batch size, rest are dimensions of data.\n",
    "# Numbers are based on input shape of MTT ReLU layers\n",
    "# shape = (256, 128, 32, 32) # Beware of stack overflow for big tensors, used vectors in C++!\n",
    "shape = (4, 3, 2, 2)\n",
    "\n",
    "x = torch.randn(*shape, dtype=torch.float, requires_grad=True)\n",
    "relu = nn.ReLU()\n",
    "out = relu(x)  # Forward pass\n",
    "\n",
    "target = torch.randn(*shape)  # Imaginary target values, defines shape of dout\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(out, target)  # Compute loss\n",
    "loss.backward()  # Backward pass\n",
    "\n",
    "# Validation\n",
    "dout = 2 * (out - target) / np.prod(shape) # derivative of MSELoss\n",
    "cache = x.clone().detach() # Must clone x so it isn't overwritten\n",
    "cache[cache <= 0] = 0\n",
    "dx = cache\n",
    "dx[dx > 0] = 1\n",
    "dx_truth = dout * dx\n",
    "print(f\"x gradients are approximately the same: {torch.allclose(dx_truth, x.grad)}\")\n",
    "\n",
    "\n",
    "# File name: shape_variable_testNum\n",
    "test_folder = Path('../../mtt_components/relu/tests')\n",
    "name = f'relu_{shape[0]}_{shape[1]}_{shape[2]}_{shape[3]}'\n",
    "it = 1\n",
    "npy_arrays = {\n",
    "    'x' : x.detach().numpy(),\n",
    "    'dx' : x.grad.detach().numpy(),\n",
    "    'out' : out.detach().numpy(),\n",
    "    'dout' : dout.detach().numpy(),\n",
    "}\n",
    "\n",
    "save_var(test_folder, name, npy_arrays, it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Affine/Linear layer testdata generation (using arange to create specific weights)\n",
    "\n",
    "# in_shape = (3, 2, 2) # First val is batch size, rest are input dimensions\n",
    "# layer_shape = (np.prod(in_shape[1:]), 14) # Shape of linear layer, in_features and out_features\n",
    "\n",
    "# x = torch.arange(np.prod(in_shape), dtype=torch.float, requires_grad=True)\n",
    "# x = x.reshape((in_shape[0], np.prod(in_shape[1:]))) # Reshape to shape [batch, prod(input_shape)]\n",
    "# x.retain_grad() # Needed to calculate gradients for tensor views, i.e. reshape\n",
    "\n",
    "# linear = nn.Linear(*layer_shape)\n",
    "# w = reversed(torch.arange(np.prod(linear.weight.shape), dtype=torch.float))\n",
    "# w = w.reshape(linear.weight.shape)\n",
    "# linear.weight = torch.nn.Parameter(w)\n",
    "# b = torch.full(linear.bias.shape, 2, dtype=torch.float)\n",
    "# linear.bias = torch.nn.Parameter(b)\n",
    "# out = linear(x) # Forward pass\n",
    "\n",
    "# target = torch.randn(in_shape[0], layer_shape[1])  # Imaginary target values, defines shape of dout\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss = loss_fn(out, target)  # Compute loss\n",
    "# loss.backward()\n",
    "\n",
    "# # Validation\n",
    "# dout = 2 * (out - target) / np.prod((in_shape[0], layer_shape[1])) # derivative of MSELoss\n",
    "# dx_truth = dout @ w\n",
    "# dw_truth = dout.T @ x\n",
    "# db_truth = torch.sum(dout.T, axis=1)\n",
    "# print(f\"x gradients are approximately the same: {torch.allclose(dx_truth, x.grad)}\")\n",
    "# print(f\"w gradients are approximately the same: {torch.allclose(dw_truth, linear.weight.grad)}\")\n",
    "# print(f\"b gradients are approximately the same: {torch.allclose(db_truth, linear.bias.grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x gradients are approximately the same: True\n",
      "w gradients are approximately the same: True\n",
      "b gradients are approximately the same: True\n",
      "out values are approximately the same: True\n",
      "Saved variable x to ../../mtt_components/linear_forward/tests\n",
      "Saved variable dx to ../../mtt_components/linear_forward/tests\n",
      "Saved variable w to ../../mtt_components/linear_forward/tests\n",
      "Saved variable dw to ../../mtt_components/linear_forward/tests\n",
      "Saved variable b to ../../mtt_components/linear_forward/tests\n",
      "Saved variable db to ../../mtt_components/linear_forward/tests\n",
      "Saved variable out to ../../mtt_components/linear_forward/tests\n",
      "Saved variable dout to ../../mtt_components/linear_forward/tests\n"
     ]
    }
   ],
   "source": [
    "# Affine/Linear layer testdata generation (random tensors)\n",
    "# Remember to make dimensions divisible by block shape (usually 2^n)\n",
    "\n",
    "in_shape = (8, 3, 2, 2) # First val is batch size, rest are input dimensions\n",
    "layer_shape = (np.prod(in_shape[1:]), 4) # Shape of linear layer, in_features and out_features\n",
    "\n",
    "x = torch.randn(*in_shape, requires_grad=True)\n",
    "x = x.reshape((in_shape[0], np.prod(in_shape[1:]))) # Reshape to shape [batch, prod(input_shape)]\n",
    "x.retain_grad() # Needed to calculate gradients for tensor views, i.e. reshape\n",
    "\n",
    "linear = nn.Linear(*layer_shape)\n",
    "out = linear(x) # This output will also be our dout, since loss is just a sum\n",
    "\n",
    "target = torch.randn(in_shape[0], layer_shape[1])  # Imaginary target values, defines shape of dout\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(out, target)  # Compute loss\n",
    "loss.backward()\n",
    "\n",
    "# Validation (TODO: rename variables to val, not truth)\n",
    "dout = 2 * (out - target) / np.prod((in_shape[0], layer_shape[1])) # derivative of MSELoss\n",
    "dx_truth = dout @ linear.weight\n",
    "dw_truth = dout.T @ x\n",
    "db_truth = torch.sum(dout, axis=0)\n",
    "out_truth = x @ linear.weight.T + linear.bias\n",
    "print(f\"x gradients are approximately the same: {torch.allclose(dx_truth, x.grad)}\")\n",
    "print(f\"w gradients are approximately the same: {torch.allclose(dw_truth, linear.weight.grad)}\")\n",
    "print(f\"b gradients are approximately the same: {torch.allclose(db_truth, linear.bias.grad)}\")\n",
    "print(f\"out values are approximately the same: {torch.allclose(out_truth, out)}\")\n",
    "\n",
    "\n",
    "test_folder = Path('../../mtt_components/linear_forward//tests')\n",
    "name = f'in_{in_shape[0]}_{in_shape[1]}_{in_shape[2]}_{in_shape[3]}_out_{layer_shape[1]}'\n",
    "it = 1\n",
    "npy_arrays = {\n",
    "    'x' : x.detach().numpy(),\n",
    "    'dx' : x.grad.detach().numpy(),\n",
    "    'w' : linear.weight.detach().numpy(),\n",
    "    'dw' : linear.weight.grad.detach().numpy(),\n",
    "    'b' : linear.bias.detach().numpy(),\n",
    "    'db' : linear.bias.grad.detach().numpy(),\n",
    "    'out' : out.detach().numpy(),\n",
    "    'dout' : dout.detach().numpy()\n",
    "}\n",
    "\n",
    "save_var(test_folder, name, npy_arrays, it)\n",
    "\n",
    "# # File name: shape_variable_testNum\n",
    "# name = f'in_{in_shape[0]}_{in_shape[1]}_{in_shape[2]}_{in_shape[3]}_out_{layer_shape[1]}'\n",
    "# with open(f'linear_{name}_x_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, x.detach().numpy())\n",
    "# with open(f'linear_{name}_w_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, linear.weight.detach().numpy())\n",
    "# with open(f'linear_{name}_b_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, linear.bias.detach().numpy())\n",
    "# with open(f'linear_{name}_out_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, out.detach().numpy())\n",
    "# with open(f'linear_{name}_dx_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, x.grad.detach().numpy())\n",
    "# with open(f'linear_{name}_dw_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, linear.weight.grad.detach().numpy())\n",
    "# with open(f'linear_{name}_db_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, linear.bias.grad.detach().numpy())\n",
    "# with open(f'linear_{name}_dout_t{it}.npy', 'wb') as f:\n",
    "#     np.save(f, dout.detach().numpy()) # Note this is what we computed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses are approximately the same: True\n",
      "out gradients are approximately the same: True\n",
      "Saved variable y_truth to ../../mtt_components/CE_forward/tests\n",
      "Saved variable y_out to ../../mtt_components/CE_forward/tests\n",
      "Saved variable loss to ../../mtt_components/CE_forward/tests\n",
      "Saved variable dout to ../../mtt_components/CE_forward/tests\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy loss function\n",
    "\n",
    "in_shape = (4, 10) # Shape [N, C] N= batch, C = channels (logits, i.e. preactivation)\n",
    "\n",
    "# PyTorch\n",
    "out = torch.randn(in_shape, requires_grad=True) # Imaginary output of model, logits\n",
    "target = torch.empty(in_shape[0], dtype=torch.long).random_(10)  # Imaginary target values. Must be type torch.long for CE function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Loss function, default reduction is 'mean'\n",
    "loss_truth = loss_fn(out, target)  # Compute loss\n",
    "loss_truth.backward() # Compute gradient\n",
    "dout_truth = out.grad # PyTorch dout\n",
    "\n",
    "# Validation\n",
    "y_out = out.detach().numpy() # Logits are not whole numbers!\n",
    "y_truth = target.detach().numpy().astype(np.int64) # Equivalent to torch.long\n",
    "\n",
    "# Forward pass\n",
    "y_truth_one_hot = np.zeros_like(y_out, dtype=int) # Same shape as model output\n",
    "y_truth_one_hot[np.arange(y_out.shape[0]), y_truth] = 1 # Assign index of label as 1, others are 0\n",
    "y_out_exp = np.exp(y_out - np.max(y_out, axis=1, keepdims=True))\n",
    "y_out_probs = y_out_exp / np.sum(y_out_exp, axis=1, keepdims=True) # Cached for backwards\n",
    "loss_val = -y_truth_one_hot * np.log(y_out_probs)\n",
    "loss_val = loss_val.sum(axis=1).mean()\n",
    "\n",
    "# Backwards pass\n",
    "dout_val = y_out_probs\n",
    "dout_val[np.arange(y_out.shape[0]), y_truth] -= 1\n",
    "dout_val /= y_out.shape[0] # Hand calculated dout to validate\n",
    "\n",
    "print(f\"Losses are approximately the same: {np.allclose(loss_val, loss_truth.detach().numpy())}\")\n",
    "print(f\"out gradients are approximately the same: {np.allclose(dout_val, dout_truth.detach().numpy())}\")\n",
    "\n",
    "# File name: shape_variable_testNum\n",
    "test_folder = Path('../../mtt_components/CE_forward/tests')\n",
    "name = f'CE_in_{in_shape[0]}_{in_shape[1]}'\n",
    "it = 1\n",
    "npy_arrays = {\n",
    "    'y_truth' : y_truth.astype(np.float32), # Convert to float for HLS\n",
    "    'y_out' : x.grad.detach().numpy(),\n",
    "    'loss' : loss_truth.detach().numpy(),\n",
    "    'dout' : dout_truth.detach().numpy()\n",
    "}\n",
    "\n",
    "save_var(test_folder, name, npy_arrays, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00193277  0.00477011  0.00308699  0.00672569  0.09920373  0.03577068\n",
      "   0.05166655  0.03266254 -0.24683122  0.01101214]\n",
      " [ 0.16031457  0.01290597  0.03026139  0.00292414  0.00321966  0.00724543\n",
      "  -0.24004994  0.01285327  0.00656767  0.00375785]\n",
      " [ 0.02543066 -0.24391416  0.01562056  0.05151001  0.0659417   0.01045887\n",
      "   0.01792915  0.02642985  0.01992954  0.01066382]\n",
      " [ 0.01856777  0.00481262  0.04748836  0.00946206  0.00825876  0.02232138\n",
      "   0.02003792  0.03949658 -0.23803462  0.06758916]]\n",
      "tensor([[ 0.0019,  0.0048,  0.0031,  0.0067,  0.0992,  0.0358,  0.0517,  0.0327,\n",
      "         -0.2468,  0.0110],\n",
      "        [ 0.1603,  0.0129,  0.0303,  0.0029,  0.0032,  0.0072, -0.2400,  0.0129,\n",
      "          0.0066,  0.0038],\n",
      "        [ 0.0254, -0.2439,  0.0156,  0.0515,  0.0659,  0.0105,  0.0179,  0.0264,\n",
      "          0.0199,  0.0107],\n",
      "        [ 0.0186,  0.0048,  0.0475,  0.0095,  0.0083,  0.0223,  0.0200,  0.0395,\n",
      "         -0.2380,  0.0676]])\n"
     ]
    }
   ],
   "source": [
    "print(dout_val)\n",
    "print(dout_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution outputs are similar True\n",
      "Gradient of weights are similar True\n",
      "Gradient of bias are similar True\n"
     ]
    }
   ],
   "source": [
    "# Convolution layer testdata generation\n",
    "\n",
    "padding = 1\n",
    "stride = 1\n",
    "kernel = 3\n",
    "bias = True\n",
    "# Input dimensions (N, Cin, Hin, Win)\n",
    "N, Cin, Hin, Win = (7,3,5,5)\n",
    "# Output dimensions (N, Cout, Hout, Wout)\n",
    "Cout = 4\n",
    "Hout = int(1 + (Hin + 2 * padding - kernel)/stride)\n",
    "Wout = int(1 + (Win + 2 * padding - kernel)/stride)\n",
    "\n",
    "# Truth\n",
    "x = torch.randn((N, Cin, Hin, Win), requires_grad=True)\n",
    "conv = nn.Conv2d(Cin, Cout, kernel_size=kernel, stride=stride, padding=padding, bias=bias)\n",
    "out_truth = conv(x)\n",
    "out_truth.retain_grad() # Needed to retain gradient during autograd\n",
    "\n",
    "# print(Hout)\n",
    "# print(Wout)\n",
    "# print(out_truth.shape)\n",
    "\n",
    "# Validation\n",
    "\n",
    "weights = conv.weight.detach().numpy()\n",
    "bias = conv.bias.detach().numpy()\n",
    "x_padded = np.pad(x.detach().numpy(), padding, pad_with, padder=0) # Pads all dimensions, wasteful\n",
    "x_padded = x_padded[padding:-padding, padding:-padding] # Strip padding on N and C dimensions\n",
    "# # print(x_padded.shape)\n",
    "# # print(x_padded)\n",
    "\n",
    "# Forward pass\n",
    "out_val = np.zeros((N, Cout, Hout, Wout))\n",
    "# print(out_val.shape)\n",
    "\n",
    "for n in range(N): # Iterate over batch\n",
    "    for c in range(Cout): # Iterate over out kernels\n",
    "        for h in range(Hout):\n",
    "            for w in range(Wout):\n",
    "                # print(f\"({h}, {w})\")\n",
    "                out_val[n, c, h, w] = np.sum(x_padded[n, :, h * stride: h * stride + kernel, w * stride : w * stride + kernel] * weights[c]) + bias[c]\n",
    "\n",
    "out_truth_npy = out_truth.detach().numpy() # Convert to Numpy array\n",
    "# Out values are sometimes very small, use a more relaxed relative tolerance (0.1% tolerance)\n",
    "print(f\"Convolution outputs are similar {np.allclose(out_truth_npy, out_val, rtol=1e-3, atol=1e-5)}\")\n",
    "\n",
    "# Backward pass\n",
    "# Calculate loss through linear layer\n",
    "lin_out = 7 # Number of out channels, for testing\n",
    "flatten = nn.Flatten()\n",
    "flattened = flatten(out_truth)\n",
    "linear = nn.Linear(Cout * Hout * Wout, lin_out)\n",
    "lin_out = linear(flattened)\n",
    "target = torch.randn(lin_out.shape)  # Imaginary target values\n",
    "loss_fn = nn.CrossEntropyLoss() # Loss function, default reduction is 'mean'\n",
    "loss_truth = loss_fn(lin_out, target)  # Compute loss\n",
    "loss_truth.backward() # Compute gradient\n",
    "dout_truth = out_truth.grad.detach() # Incoming gradient into conv layer, not linear!\n",
    "# print(out_truth.shape)\n",
    "# print(dout_truth.shape)\n",
    "\n",
    "# Useful numpy arrays\n",
    "\n",
    "x_npy = x.detach().numpy()\n",
    "dout_npy = dout_truth.detach().numpy()\n",
    "dw_truth = conv.weight.grad.detach().numpy()\n",
    "db_truth = conv.bias.grad.detach().numpy()\n",
    "dx_truth = x.grad.detach().numpy()\n",
    "\n",
    "# x shape [N, Cin, Hin, Win]\n",
    "# x padded shape [N, Cin, Hin + padding, Win + padding]\n",
    "# dw shape [Cout, Cin, kernel, kernel]\n",
    "# dout shape [N, Cout, Hout, Wout]\n",
    "\n",
    "# Calculate dw\n",
    "\n",
    "dw = np.zeros(weights.shape)\n",
    "\n",
    "# Loops through dw (note that loops do batch N in parallel)\n",
    "for cout in range(Cout):\n",
    "    for cin in range(Cin):\n",
    "        for h in range(kernel):\n",
    "            for w in range(kernel):\n",
    "                x_slice = x_padded[:, cin, h:h + Hout * stride:stride, w:w + Wout * stride:stride]\n",
    "                dout_slice = dout_npy[:, cout]\n",
    "                dw[cout, cin, h, w] = np.sum(x_slice * dout_slice)\n",
    "\n",
    "print(f\"Gradient of weights are similar {np.allclose(dw, dw_truth, rtol=1e-3, atol=1e-5)}\")\n",
    "\n",
    "# Calculate db\n",
    "\n",
    "db = np.zeros(bias.shape)\n",
    "for c in range(Cout):\n",
    "    db[c] = np.sum(dout_npy[:, c, :, :])\n",
    "\n",
    "print(f\"Gradient of bias are similar {np.allclose(db, db_truth, rtol=1e-3, atol=1e-5)}\")\n",
    "\n",
    "# Calculate dx\n",
    "\n",
    "dx = np.zeros(dx_truth.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28572786\n",
      "0.2857277989387512\n",
      "5.960464477539063e-08\n"
     ]
    }
   ],
   "source": [
    "# print(dw[0][-1])\n",
    "# print(dw_truth[0][-1])\n",
    "\n",
    "# print(out_truth_npy[0][-1][0][18])\n",
    "# print(out_val[0][-1][0][18])\n",
    "\n",
    "# print(out_truth_npy[0][-1][0][18]  - out_val[0][-1][0][18])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
