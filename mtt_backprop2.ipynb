{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "from torchviz import make_dot\n",
    "\n",
    "from src.reparam_module import ReparamModule\n",
    "\n",
    "# These are our reference variables that we wish to keep, to compare between models\n",
    "\n",
    "# Input batch\n",
    "# N = 7 # No batch for now, just a single variable\n",
    "\n",
    "in_shape = (2, 3) # Batch, features\n",
    "l1 = (in_shape[1], 4) # Shape of Linear Layer\n",
    "\n",
    "class TestLinear(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # No bias because in MTT, we are using normalization layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=l1[0], out_features=l1[1], bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Avoid using these directly, instead use deepcopies\n",
    "_model = TestLinear()\n",
    "num_params = sum(np.prod(l.weight.shape) for l in _model.layers)\n",
    "_img_syn = torch.randn(*in_shape).requires_grad_(True)  # Input of network\n",
    "_student_params = torch.cat(\n",
    "    [l.weight.reshape(-1) for l in _model.layers]\n",
    ").requires_grad_(\n",
    "    True\n",
    ")  # Imaginary parameters, in MTT sometimes called forward_param. These are the weights!\n",
    "starting_params = torch.randn(num_params)  # Imaginary starting expert param\n",
    "target_params = torch.randn(num_params)  # Imaginary ending expert param\n",
    "y_hat = torch.empty(in_shape[0], dtype=torch.long).random_(4) # Imaginary targets, shape output of final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2880, grad_fn=<DivBackward0>)\n",
      "tensor([[ 0.0016,  0.0010,  0.0007],\n",
      "        [-0.0064, -0.0011, -0.0023]])\n"
     ]
    }
   ],
   "source": [
    "# MTT Ground Truth, we are trying to replicate the input image gradients\n",
    "\n",
    "# Deep copies, so we know not used by manual implementation\n",
    "img_syn = copy.deepcopy(_img_syn)\n",
    "# I don't think it is possible to do a deepcopy as this removes dependencies\n",
    "student_params = _student_params\n",
    "student_params.retain_grad()\n",
    "student_net_mtt = _model\n",
    "student_net_mtt = ReparamModule(student_net_mtt)  # MTT black magic\n",
    "\n",
    "optimizer_img = torch.optim.SGD([img_syn], lr=0.1, momentum=0.5)\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "student_net_mtt.train()\n",
    "\n",
    "out = student_net_mtt(img_syn, flat_param=student_params) # Forward pass\n",
    "out.retain_grad() # For validation\n",
    "criterion = nn.CrossEntropyLoss()  # Used to calculate gradients of parameters\n",
    "ce_loss = criterion(out, y_hat) # NllLossBackward0\n",
    "ce_loss.retain_grad() # For validation\n",
    "\n",
    "# Computes graph for student_params, but does not populate the .grad attributes!\n",
    "# Returns a tuple with only a single term\n",
    "grad = torch.autograd.grad(ce_loss, student_params, create_graph=True)[0] # CatBackward0\n",
    "grad.retain_grad()\n",
    "\n",
    "# Update student params (instead of using torch.optim.SGD)\n",
    "student_params_new = (student_params - 0.1 * grad) # SubBackward0, MulBackward0, CatBackward0, ViewBackward0,\n",
    "student_params_new.retain_grad() # For validation\n",
    "\n",
    "param_loss = torch.nn.functional.mse_loss(student_params_new, target_params, reduction=\"sum\") # MseLossBackward0\n",
    "param_loss.retain_grad() # For validation\n",
    "param_dist = torch.nn.functional.mse_loss(starting_params, target_params, reduction=\"sum\")\n",
    "\n",
    "# Believe this is their manual way of taking the mean lol. But this isn't actually needed\n",
    "# because they have done num_param / num_param = 1\n",
    "# param_loss /= num_params\n",
    "# param_dist /= num_params\n",
    "\n",
    "# MTT grand loss, with the full computational graph determined by autograd\n",
    "grand_loss = param_loss / param_dist\n",
    "grand_loss.retain_grad() # For validation\n",
    "\n",
    "# Before backwards pass of grand loss, gradient should be zero\n",
    "assert img_syn.grad == None\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "# Now do backprop, and calculate\n",
    "grand_loss.backward(retain_graph=True) # retain_graph for torchviz\n",
    "\n",
    "print(grand_loss)\n",
    "print(img_syn.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # graph = make_dot(loss_grand, show_attrs=True) # Verbose, requires retaining graph\n",
    "# graph = make_dot(grand_loss, show_attrs=True)\n",
    "# # Img may be cut off, hence save to a file\n",
    "# graph.render(\"img/mtt_grand\", view=True, format=\"pdf\")\n",
    "# # graph\n",
    "\n",
    "# # graph = make_dot(loss_grand, show_attrs=True) # Verbose, requires retaining graph\n",
    "# graph = make_dot(grad, show_attrs=True)\n",
    "# # Img may be cut off, hence save to a file\n",
    "# graph.render(\"img/grad\", view=True, format=\"pdf\")\n",
    "# # graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3276, 0.1205, 0.4679, 0.0840],\n",
      "        [0.2291, 0.1715, 0.5255, 0.0739]], grad_fn=<DivBackward0>)\n",
      "tensor([0, 1])\n",
      "tensor([[ 0.3276,  0.1205, -0.5321,  0.0840],\n",
      "        [ 0.2291,  0.1715,  0.5255, -0.9261]], grad_fn=<IndexPutBackward0>)\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# let us calculate the forward computational graph manually\n",
    "\n",
    "# print(ce_loss)\n",
    "\n",
    "# Calculate CELoss forward\n",
    "flat_reshaped = torch.reshape(student_params, (4,3))\n",
    "# y_truth_one_hot = torch.zeros_like(out, dtype=int)  # Same shape as model output\n",
    "y_truth_one_hot = torch.zeros_like(img_syn @ flat_reshaped.T, dtype=int)  # This is the forward pass of normal DNN\n",
    "y_truth_one_hot[torch.arange(out.shape[0]), y_hat] = (\n",
    "    1  # Assign index of label as 1, others are 0\n",
    ")\n",
    "y_max = torch.max(out, axis=1, keepdims=True)[0]\n",
    "y_out_exp = torch.exp(out - y_max)\n",
    "y_out_probs = y_out_exp / torch.sum(y_out_exp, axis=1, keepdims=True) # Cached for backwards\n",
    "loss_val = -y_truth_one_hot * torch.log(y_out_probs)\n",
    "loss_val = loss_val.sum(axis=1).mean()\n",
    "assert torch.allclose(loss_val, ce_loss)\n",
    "# print(y_out_probs.shape)\n",
    "# Here we are forward passing the backprop of CELoss (yes, this will confuse you)\n",
    "dout_val = y_out_probs\n",
    "print(dout_val)\n",
    "print(torch.arange(out.shape[0]))\n",
    "dout_val[torch.arange(out.shape[0]), y_hat] -= 1\n",
    "print(dout_val)\n",
    "dout_val /= out.shape[0]  # Hand calculated dout to validate\n",
    "\n",
    "print(dout_val.shape)\n",
    "\n",
    "# # Calculate MmBackward0 forward (the 'hidden' layer)\n",
    "MmForward = dout_val.T @ img_syn\n",
    "MmForward = MmForward.view(-1)\n",
    "assert torch.allclose(MmForward, grad)\n",
    "_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[ 0.0030,  0.0004,  0.0008],\n",
      "        [-0.0061, -0.0013, -0.0025]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.0016,  0.0010,  0.0007],\n",
      "        [-0.0064, -0.0011, -0.0023]])\n"
     ]
    }
   ],
   "source": [
    "# Now let us calculate img_syn.grad manually\n",
    "\n",
    "# dparam_loss (MSELoss wrt student_params_new)\n",
    "dMSE = 2 * (student_params_new - target_params) / param_dist\n",
    "# print(student_params.grad)\n",
    "# print(dparam_loss)\n",
    "# print(student_params.grad - dparam_loss)\n",
    "assert(torch.allclose(student_params_new.grad, dMSE))\n",
    "\n",
    "# dstudent_params_grad\n",
    "# Take derivative of student_param_new wrt grad * incoming gradient\n",
    "# This is the gradient that leaves the first MulBackward0 at bottom of graph\n",
    "dstudent_params_grad = -0.1 * dMSE\n",
    "# print(grad.grad)\n",
    "# print(d_student_params)\n",
    "# print(grad.grad- d_student_params)\n",
    "assert(torch.allclose(grad.grad, dstudent_params_grad))\n",
    "\n",
    "# dstudent_params_student_params (incoming grad. wrt student_params)\n",
    "# Take derivative of student_param wrt student_param * incoming gradient\n",
    "# TODO: This is dparam_loss plus whatever the other incoming gradient is (accumulate)\n",
    "# This is the gradient that enters the CatBackward0 at the top of the graph\n",
    "# dstudent_params_student_params = dparam_loss\n",
    "# assert(torch.allclose(student_params.grad, dstudent_params_student_params))\n",
    "\n",
    "# dMmBackward0 (hidden layer)\n",
    "\n",
    "# Calculate backprop through the CELoss layer\n",
    "# back = dstudent_params_grad / out.shape[0]\n",
    "# back = torch.reshape(back, (3,4))\n",
    "# # back[torch.arange(out.shape[0]), y_hat] -= 1\n",
    "# print(back.shape)\n",
    "# TODO: Figure out to backprop through the backprop of CELoss\n",
    "dtmp = img_syn @ torch.reshape(dstudent_params_grad, (4,3)).T # Should be same shape as dout_val (2,4)\n",
    "dtmp /= out.shape[0]\n",
    "\n",
    "print(dtmp.shape)\n",
    "# dtmp = dtmp @ flat_reshaped\n",
    "\n",
    "\n",
    "# Incoming gradient is dstudent_params_grad or grad.grad\n",
    "# d_hidden = dout_val @ torch.reshape(grad.grad, (4,3))\n",
    "d_hidden = dout_val @ torch.reshape(dstudent_params_grad, (4,3))\n",
    "print(d_hidden)\n",
    "print(img_syn.grad)\n",
    "# print(img_syn.grad - dtmp - d_hidden) # Trying to get this to zero\n",
    "\n",
    "# dgrad (incoming grad. wrt grad)\n",
    "# TODO: This must be examined in detail, check why it is 1\n",
    "# Think it is because we backprop CE itself, not its gradient\n",
    "# print(ce_loss.grad)\n",
    "# dgrad = torch.tensor([1.0])\n",
    "# # print(ce_loss.grad - dgrad)\n",
    "# assert(torch.allclose(ce_loss.grad, dgrad))\n",
    "\n",
    "# dstudent_params wrt grad\n",
    "# First we have to calculate this weird 'hidden' layer\n",
    "\n",
    "# dhidden = torch.ones_like(grad) * -0.1\n",
    "# print(dhidden)\n",
    "\n",
    "# dCELoss\n",
    "# out is the last MMBackward0 before the AccumulateGrad for x\n",
    "\n",
    "# print(out.shape)\n",
    "# print(out - d_student_params.reshape([4,3]))\n",
    "# out_test = out - img_syn @ torch.reshape(d_student_params, (3,4))\n",
    "\n",
    "# out_exp = torch.exp(out - torch.max(out, axis=1, keepdims=True)[0])\n",
    "# y_out_probs = out_exp / torch.sum(out_exp, axis=1, keepdims=True) # Forwards caches this\n",
    "# dce_loss = y_out_probs\n",
    "# dce_loss[torch.arange(out.shape[0]), y_hat] -= 1\n",
    "# dce_loss /= out.shape[0]\n",
    "\n",
    "# out_npy = out.detach().numpy()\n",
    "# y_hat = y_hat.numpy()\n",
    "# out_exp = np.exp(out_npy - np.max(out_npy, axis=1, keepdims=True)[0])\n",
    "# y_out_probs = out_exp / np.sum(out_exp, axis=1, keepdims=True) # Forwards caches this\n",
    "# dce_loss = y_out_probs\n",
    "# dce_loss[np.arange(out_npy.shape[0]), y_hat] -= 1.0\n",
    "# dce_loss /= out_npy.shape[0]\n",
    "# print(dce_loss - out.grad.detach().numpy())\n",
    "\n",
    "# print(dce_loss - out.grad)\n",
    "# print(img_syn)\n",
    "# print(dce_loss / out.grad)\n",
    "# assert(torch.allclose(out.grad, dce_loss)) # The difference here is quite large, maybe torch calculates differently?\n",
    "\n",
    "# dout (can't calculate it directly yet)\n",
    "\n",
    "# print(out.grad.shape)\n",
    "# print(torch.reshape(student_params, [4,3]).shape)\n",
    "\n",
    "# dhidden_truth = out.grad - img_syn.grad\n",
    "# print(dhidden_truth)\n",
    "\n",
    "\n",
    "\n",
    "_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grad)\n",
    "\n",
    "# current = grad\n",
    "\n",
    "# while current.grad_fn is not None:\n",
    "#     print(current.grad_fn)\n",
    "#     current = current.grad_fn.next_functions[0][0]  # Move to the next function in the graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
