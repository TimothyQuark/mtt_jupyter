{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Implement ConvNet model\n",
    "- Implement loading of CIFAR10 dataset\n",
    "- Write training scheme for the buffer\n",
    "- Write training scheme for the distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Distillation by Matching Training Trajectories\n",
    "\n",
    "Implementation of the MTT method, based on the [code by the paper's authors](https://github.com/GeorgeCazenavette/mtt-distillation/tree/main). This implementation uses Jupyter notebooks to visualize the model, and is also not dependent on any online services like in the original implementation.\n",
    "\n",
    "- Model used: ConvNet (as implemented by the MTT paper)\n",
    "- Dataset used: CIFAR10, CIFAR100\n",
    "\n",
    "To get started, you can run the notebook as is, or modify settings the User Settings section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cells will import important packages, configure devices, load TensorBoard etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "# TODO: this should be the only switch to decide if we use CUDA or CPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device being used: {device}\")\n",
    "\n",
    "# For debugging CUDA, we can record memory history\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.memory._record_memory_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Matplotlib so that it properly visualizes inside Jupyter notebook using VSCode\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some small fixes to prevent Jupyter from bugging out\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To prevent the kernel from dying.\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12721), started 0:37:16 ago. (Use '!kill 12721' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6a5e8f66263d822f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6a5e8f66263d822f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up TensorBoard\n",
    "\n",
    "\"\"\" Notes on superbuggy VSCode tensorboard\n",
    "\n",
    "The general conclusion is that Tensorboard in VSCode breaks regularly for whatever reasons, and nobody\n",
    "cares to fix it. So just use Tensboard from the browser. Code below will run tensorboard on port 6006\n",
    "\n",
    "For some reason tensorboard isn't quitting properly, and keeps the port occupied even\n",
    "if the process itself is killed. Maybe it will be patched someday...\n",
    "\n",
    "1. Check Code Python Interpreter is correct\n",
    "\n",
    "2. Check if port occupied:\n",
    "lsof -i:6006\n",
    "\n",
    "Kill using the PID from above command (not 6006, the PID!):\n",
    "kill PID\n",
    "\n",
    "See https://stackoverflow.com/questions/54395201/tensorboard-could-not-bind-to-port-6006-it-was-already-in-use\n",
    "Also, see this on how to kill tensorboard process (but not the problem here): https://stackoverflow.com/questions/36896164/tensorflow-how-to-close-tensorboard-server\n",
    "\n",
    "3. Alternative if you want to see tensorboard in a webbrowser (6006, 8888 etc):\n",
    "tensorboard --logdir=data/ --host localhost --port 6006\n",
    "http//localhost:6006\n",
    "\n",
    "Another possible bug is that installing tensorboard with conda is not visible in Jupyter Notebooks using VSCode. You will have to reinstall it using pip instead\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Settings\n",
    "\n",
    "Here you can define the most important aspects of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "\n",
    "# User settings:\n",
    "settings = {\n",
    "    \"dataset\": \"CIFAR10\",  # [CIFAR10, CIFAR100]\n",
    "    \"model\": \"ConvNet\",  # [ConvNet, ResNet]\n",
    "    \"ZCA\": False,  # [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    # General hyperparameters\n",
    "    \"device\": device,\n",
    "    \"num_workers\": 16,\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 16,\n",
    "    \"max_patience\": 3,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"momentum\": 0.0,\n",
    "    # Model specific hyperparameters\n",
    "    \"in_channels\": 3,\n",
    "    \"out_channels\": 10,\n",
    "    \"net_depth\": 3,\n",
    "    \"net_width\": 128,\n",
    "    \"kernel_size\": 3,\n",
    "    \"padding\": 1,\n",
    "    \"activation\": nn.ReLU(inplace=True),\n",
    "    \"norm\": nn.GroupNorm(\n",
    "        num_groups=3, num_channels=3, affine=True\n",
    "    ),  # group and channels should be equal to in_channels? Check\n",
    "    \"pooling\": nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    \"optimizer\": \"SGD\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare dataset\n",
    "\n",
    "The models used in this notebook are already provided by PyTorch itself, so we can simply download and modify them. However, the PyTorch class not accept slice indexes, so we will have to write our own little version on top of it which fixes the __getitem__ method. That way, we can do something like train_dataset[0:3] and not just train_dataset[1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "\n",
    "\n",
    "class FixedCIFAR10(CIFAR10):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str | Path,\n",
    "        train: bool = True,\n",
    "        transform: Callable[..., Any] | None = None,\n",
    "        target_transform: Callable[..., Any] | None = None,\n",
    "        download: bool = False,\n",
    "    ) -> None:\n",
    "        # Initialization does not need to be changed\n",
    "        super().__init__(root, train, transform, target_transform, download)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            return [self[i] for i in range(*key.indices(len(self)))]\n",
    "        else:\n",
    "            return super().__getitem__(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset (CIFAR10)\n",
      "Files already downloaded and verified\n",
      "Loading testing dataset (CIFAR10)\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_dataset = None\n",
    "# val_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "if settings[\"dataset\"] == \"CIFAR10\":\n",
    "    if settings[\"ZCA\"]:\n",
    "        raise NotImplementedError(\"ZCA not yet implemented\")\n",
    "    else:\n",
    "        # Normalize CIFAR10 dataset (Note: not same as those used in MTT repo, these were calculated directly from\n",
    "        # the PyTorch CIFAR10 dataset itself)\n",
    "        mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "        std = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]\n",
    "        )\n",
    "\n",
    "    print(f\"Loading training dataset ({settings[\"dataset\"]})\")\n",
    "    train_dataset = FixedCIFAR10(\n",
    "        root=\"data/CIFAR10\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    print(f\"Loading testing dataset ({settings[\"dataset\"]})\")\n",
    "    test_dataset = FixedCIFAR10(\n",
    "        root=\"data/CIFAR10\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset defined by user: {settings[\"dataset\"]}\")\n",
    "\n",
    "assert train_dataset != None and test_dataset != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Test set size: 10000\n",
      "Image dimensions: torch.Size([3, 32, 32])\n",
      "Target type: <class 'int'>\n",
      "First image tensor: \n",
      " tensor([[[-1.0545e+00, -1.3085e+00, -1.1974e+00,  ...,  5.1685e-01,\n",
      "           4.2161e-01,  3.5812e-01],\n",
      "         [-1.7371e+00, -1.9910e+00, -1.7053e+00,  ..., -3.8698e-02,\n",
      "          -1.0219e-01, -5.4570e-02],\n",
      "         [-1.5942e+00, -1.7371e+00, -1.2133e+00,  ..., -1.1806e-01,\n",
      "          -8.6316e-02, -2.6091e-01],\n",
      "         ...,\n",
      "         [ 1.3105e+00,  1.1994e+00,  1.1518e+00,  ...,  5.4859e-01,\n",
      "          -1.1022e+00, -1.1498e+00],\n",
      "         [ 8.6604e-01,  7.5494e-01,  9.6128e-01,  ...,  9.2953e-01,\n",
      "          -4.5139e-01, -6.7360e-01],\n",
      "         [ 8.1843e-01,  6.7557e-01,  8.5017e-01,  ...,  1.4375e+00,\n",
      "           4.0574e-01, -3.8698e-02]],\n",
      "\n",
      "        [[-9.8258e-01, -1.2403e+00, -1.2081e+00,  ...,  1.4494e-01,\n",
      "           3.2192e-02,  1.6085e-02],\n",
      "         [-1.6591e+00, -1.9812e+00, -1.8524e+00,  ..., -5.6379e-01,\n",
      "          -6.4432e-01, -5.7989e-01],\n",
      "         [-1.5947e+00, -1.8685e+00, -1.5463e+00,  ..., -6.2822e-01,\n",
      "          -6.2822e-01, -8.0540e-01],\n",
      "         ...,\n",
      "         [ 7.5703e-01,  4.8320e-01,  6.1206e-01,  ...,  1.6105e-01,\n",
      "          -1.4819e+00, -1.4336e+00],\n",
      "         [ 2.5770e-01, -2.2891e-05,  3.3823e-01,  ...,  4.0266e-01,\n",
      "          -9.8258e-01, -1.1275e+00],\n",
      "         [ 3.3823e-01,  9.6622e-02,  3.0602e-01,  ...,  9.8254e-01,\n",
      "          -8.0560e-02, -4.9936e-01]],\n",
      "\n",
      "        [[-7.6367e-01, -1.0336e+00, -1.0636e+00,  ..., -8.8763e-02,\n",
      "          -1.7875e-01, -1.6375e-01],\n",
      "         [-1.4086e+00, -1.7085e+00, -1.7085e+00,  ..., -8.8365e-01,\n",
      "          -9.5864e-01, -8.5365e-01],\n",
      "         [-1.3936e+00, -1.7085e+00, -1.5885e+00,  ..., -9.5864e-01,\n",
      "          -9.5864e-01, -1.0786e+00],\n",
      "         ...,\n",
      "         [-2.6874e-01, -1.1986e+00, -1.3186e+00,  ..., -6.5868e-01,\n",
      "          -1.6035e+00, -1.4086e+00],\n",
      "         [-2.6874e-01, -1.0786e+00, -1.2586e+00,  ..., -2.9873e-01,\n",
      "          -1.1986e+00, -1.1986e+00],\n",
      "         [ 3.1220e-02, -2.9873e-01, -4.0372e-01,  ...,  3.9117e-01,\n",
      "          -4.4871e-01, -6.2869e-01]]])\n"
     ]
    }
   ],
   "source": [
    "# Show some basic information about the dataset\n",
    "\n",
    "print(\"Training set size: %i\" % len(train_dataset))\n",
    "# print(\"Validation set size: %i\" % len(val_dataset))\n",
    "print(\"Test set size: %i\" % len(test_dataset))\n",
    "\n",
    "first_image, first_target = train_dataset[0]\n",
    "print(f\"Image dimensions: {first_image.shape}\")  # C * W * H (C = 3 because RGB)\n",
    "print(f\"Target type: {type(first_target)}\")\n",
    "print(f\"First image tensor: \\n {first_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize data\n",
    "from torch import Tensor\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# TODO: use torch.tensor.view instead of transpose when possible\n",
    "\n",
    "def visualize_imgs(images: Tensor | List[Tensor]):\n",
    "    if isinstance(images, Tensor):\n",
    "        images = [images]\n",
    "\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(20, 3 * n))\n",
    "\n",
    "    for idx, (image, targets) in enumerate(images):\n",
    "        # Normalized input image\n",
    "        plt.subplot(n, 1, idx * 1 + 1)\n",
    "        plt.imshow(image.numpy().transpose(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Input image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show some of the dataset. Note that normalization means the values are no longer in the range of 0-1, so\n",
    "# there will be clipping warnings.\n",
    "\n",
    "ex_imgs = train_dataset[0:3]\n",
    "\n",
    "visualize_imgs(ex_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Create model\n",
    "- Create tensor that uses one hot encoding for ids, not just a flat image\n",
    "- Use a UNet\n",
    "- Create trainer (use dataloader, load to device)\n",
    "- Use a good loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Classes for our model\n",
    "\n",
    "\n",
    "class ConvSandwich(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class that creates a Convolution Layer with model specific hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            hparams[\"in_channels\"],\n",
    "            hparams[\"out_channels\"],\n",
    "            hparams[\"kernel_size\"],\n",
    "            padding=hparams[\"padding\"],\n",
    "        )\n",
    "        self.norm = hparams[\"norm\"]\n",
    "        self.activation = hparams[\"activation\"]\n",
    "        self.pooling = hparams[\"pooling\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pooling(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class for debugging behaviour of model. Will print out the shape of the layer input,\n",
    "    and input passed directly to output\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        # print(\"MODEL DATA ON: \", x.get_device(), \"MODEL PARAMS ON: \", self.classifier.weight.data.get_device())\n",
    "        return x\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights for each layer of model m, based on the type of activation function\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, hparams) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hp = hparams\n",
    "        self.kernel_size = self.hp[\"kernel_size\"]  # used very often\n",
    "\n",
    "        self.features = nn.ModuleList(\n",
    "            [\n",
    "                ConvSandwich(self.hparams),\n",
    "                ConvSandwich(self.hparams),\n",
    "                ConvSandwich(self.hparams),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            self.hp[\"in_channels\"] * self.hp[\"in_channels\"] * self.hp[\"in_channels\"],\n",
    "            self.hp[\"out_channels\"],\n",
    "        )\n",
    "\n",
    "        # We want the model to be stored on GPU if possible\n",
    "        self.device = hparams[\"device\"]\n",
    "\n",
    "        self.set_optimizer()\n",
    "\n",
    "        # Apply initial weights\n",
    "        # self.encoder.apply(weights_init)\n",
    "        # self.decoder.apply(weights_init)\n",
    "        # self.classifier.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_optimizer(self):\n",
    "        self.optimizer = None\n",
    "\n",
    "        if self.hp == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(\n",
    "                self.parameters(),\n",
    "                lr=self.hp[\"learning_rate\"],\n",
    "                momentum=self.hp[\"momentum\"],\n",
    "                weight_decay=self.hp[\"weight_decay\"],\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected Optimizer chosen: {self.hp}\")\n",
    "\n",
    "    def training_step(self, batch, loss_func):\n",
    "        self.train() # Set model to training mode\n",
    "        self.optimizer.zero_grad()  # Reset gradient every batch\n",
    "\n",
    "        # N = batch size, C = channels (3 for RGB), H = image height, W = image width\n",
    "        images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "        target = (\n",
    "            batch[1][0].to(self.device).squeeze()\n",
    "        )\n",
    "\n",
    "        # Model makes prediction (forward pass)\n",
    "        pred = self.forward(images)  # N x C x H x W (C=num of CityScape classes)\n",
    "\n",
    "        # Calculate loss, do backward pass to update weights, optimizer takes step\n",
    "        # torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\") wants target to be of type long, not float\n",
    "        loss = loss_func(pred, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, loss_func):\n",
    "        loss = 0\n",
    "\n",
    "        # Set model to eval\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "            target = (\n",
    "                batch[1][0].to(self.device).squeeze()\n",
    "            )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "            pred = self.forward(images)\n",
    "            loss = loss_func(pred, target)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "- Optimizer only train encoder or decoder\n",
    "- Make my own model\n",
    "- Use PyTorch optimizations from their guide\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SegmentationNN(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hp = hparams\n",
    "        self.kernel_factor = self.hp[\n",
    "            \"kernel_factor\"\n",
    "        ]  # Reuse this value often, so don't search a Dict\n",
    "\n",
    "        \"\"\"\"\n",
    "        General structure of the model:\n",
    "\n",
    "        - encoder_block downsamples inputs and learns key features. Intermediate outputs are stored\n",
    "        and used by the decoder block\n",
    "        - decoder_block: Upsamples the low dimensional outputs of the encoder_block, essentially\n",
    "        'learns' how things should look, and reconstructs the full segmented image\n",
    "        - classifier\n",
    "        \"\"\"\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                ConvSandwich(\n",
    "                    3, self.kernel_factor, 3, 1, 1\n",
    "                ),  # RGB image input has 3 channels\n",
    "                ConvSandwich(self.kernel_factor, self.kernel_factor * 2, 3, 1, 1),\n",
    "                ConvSandwich(self.kernel_factor * 2, self.kernel_factor * 4, 3, 1, 1),\n",
    "                ConvSandwich(self.kernel_factor * 4, self.kernel_factor * 4, 3, 1, 1),\n",
    "                ConvSandwich(self.kernel_factor * 4, self.kernel_factor * 8, 3, 1, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                ConvSandwich(\n",
    "                    self.kernel_factor * (8 + 8), self.kernel_factor * 4, 3, 1, 1\n",
    "                ),\n",
    "                ConvSandwich(\n",
    "                    self.kernel_factor * (4 + 4), self.kernel_factor * 4, 3, 1, 1\n",
    "                ),\n",
    "                ConvSandwich(\n",
    "                    self.kernel_factor * (4 + 4), self.kernel_factor * 2, 3, 1, 1\n",
    "                ),\n",
    "                ConvSandwich(self.kernel_factor * (2 + 2), self.kernel_factor, 3, 1, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Conv2d(\n",
    "            self.kernel_factor * 2 + 3, self.hp[\"output_size\"], kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # Layer reused for downsampling\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bicubic\"\n",
    "        )  # Layer reused for upsampling\n",
    "\n",
    "        # self.print = PrintLayer()  # Used for debugging\n",
    "\n",
    "        # We want the model to be stored on GPU if possible\n",
    "        self.device = hparams.get(\n",
    "            \"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.set_optimizer()\n",
    "\n",
    "        # Apply initial weights\n",
    "        self.encoder.apply(weights_init)\n",
    "        self.decoder.apply(weights_init)\n",
    "        self.classifier.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unet network\n",
    "\n",
    "        # Note: need to use multiples of 2 or else rounding makes torch.cat try to concat tensors\n",
    "        # of different sizes\n",
    "\n",
    "        proc_x = self.encoder[0](x)\n",
    "        enc_1 = self.encoder[1](proc_x)\n",
    "        tmp = self.downsample(enc_1)\n",
    "        enc_2 = self.encoder[2](tmp)\n",
    "        tmp = self.downsample(enc_2)\n",
    "        enc_3 = self.encoder[3](tmp)\n",
    "        tmp = self.downsample(enc_3)\n",
    "        enc_4 = self.encoder[4](tmp)\n",
    "\n",
    "        bottleneck = self.downsample(enc_4)\n",
    "\n",
    "        tmp = self.upsample(bottleneck)\n",
    "        dec = self.decoder[0](torch.cat([tmp, enc_4], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "        dec = self.decoder[1](torch.cat([tmp, enc_3], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "        dec = self.decoder[2](torch.cat([tmp, enc_2], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "        dec = self.decoder[3](torch.cat([tmp, enc_1], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "\n",
    "        dec = torch.cat([dec, proc_x], dim=1)\n",
    "        dec = torch.cat([dec, x], dim=1)\n",
    "        x = self.classifier(dec)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_optimizer(self):\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hp[\"learning_rate\"],\n",
    "            weight_decay=self.hp[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, loss_func):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()  # Reset gradient every batch\n",
    "\n",
    "        # N = batch size, C = channels (3 for RGB), H = image height, W = image width\n",
    "        images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "        target = (\n",
    "            batch[1][0].to(self.device).squeeze()\n",
    "        )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "        # Model makes prediction (forward pass)\n",
    "        pred = self.forward(images)  # N x C x H x W (C=num of CityScape classes)\n",
    "\n",
    "        # Calculate loss, do backward pass to update weights, optimizer takes step\n",
    "        # torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\") wants target to be of type long, not float\n",
    "        loss = loss_func(pred, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, loss_func):\n",
    "        loss = 0\n",
    "\n",
    "        # Set model to eval\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "            target = (\n",
    "                batch[1][0].to(self.device).squeeze()\n",
    "            )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "            pred = self.forward(images)\n",
    "            loss = loss_func(pred, target)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# TODO: play around more with parameters to see if there is better solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is just for demonstration purposes, let us make sure the model is not too big. We want to be able to run it on something like a CPU even if we don't have access to a GPU! A parameter size of 25MB seems to be good enough to get this model working on a mid tier desktop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SegmentationNN                           [16, 35, 64, 32]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─ConvSandwich: 2-1                 [16, 8, 64, 32]           --\n",
       "│    │    └─Conv2d: 3-1                  [16, 8, 64, 32]           216\n",
       "│    │    └─BatchNorm2d: 3-2             [16, 8, 64, 32]           16\n",
       "│    │    └─LeakyReLU: 3-3               [16, 8, 64, 32]           --\n",
       "│    └─ConvSandwich: 2-2                 [16, 16, 64, 32]          --\n",
       "│    │    └─Conv2d: 3-4                  [16, 16, 64, 32]          1,152\n",
       "│    │    └─BatchNorm2d: 3-5             [16, 16, 64, 32]          32\n",
       "│    │    └─LeakyReLU: 3-6               [16, 16, 64, 32]          --\n",
       "├─MaxPool2d: 1-2                         [16, 16, 32, 16]          --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─ConvSandwich: 2-3                 [16, 32, 32, 16]          --\n",
       "│    │    └─Conv2d: 3-7                  [16, 32, 32, 16]          4,608\n",
       "│    │    └─BatchNorm2d: 3-8             [16, 32, 32, 16]          64\n",
       "│    │    └─LeakyReLU: 3-9               [16, 32, 32, 16]          --\n",
       "├─MaxPool2d: 1-4                         [16, 32, 16, 8]           --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─ConvSandwich: 2-4                 [16, 32, 16, 8]           --\n",
       "│    │    └─Conv2d: 3-10                 [16, 32, 16, 8]           9,216\n",
       "│    │    └─BatchNorm2d: 3-11            [16, 32, 16, 8]           64\n",
       "│    │    └─LeakyReLU: 3-12              [16, 32, 16, 8]           --\n",
       "├─MaxPool2d: 1-6                         [16, 32, 8, 4]            --\n",
       "├─ModuleList: 1-7                        --                        (recursive)\n",
       "│    └─ConvSandwich: 2-5                 [16, 64, 8, 4]            --\n",
       "│    │    └─Conv2d: 3-13                 [16, 64, 8, 4]            18,432\n",
       "│    │    └─BatchNorm2d: 3-14            [16, 64, 8, 4]            128\n",
       "│    │    └─LeakyReLU: 3-15              [16, 64, 8, 4]            --\n",
       "├─MaxPool2d: 1-8                         [16, 64, 4, 2]            --\n",
       "├─Upsample: 1-9                          [16, 64, 8, 4]            --\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvSandwich: 2-6                 [16, 32, 8, 4]            --\n",
       "│    │    └─Conv2d: 3-16                 [16, 32, 8, 4]            36,864\n",
       "│    │    └─BatchNorm2d: 3-17            [16, 32, 8, 4]            64\n",
       "│    │    └─LeakyReLU: 3-18              [16, 32, 8, 4]            --\n",
       "├─Upsample: 1-11                         [16, 32, 16, 8]           --\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvSandwich: 2-7                 [16, 32, 16, 8]           --\n",
       "│    │    └─Conv2d: 3-19                 [16, 32, 16, 8]           18,432\n",
       "│    │    └─BatchNorm2d: 3-20            [16, 32, 16, 8]           64\n",
       "│    │    └─LeakyReLU: 3-21              [16, 32, 16, 8]           --\n",
       "├─Upsample: 1-13                         [16, 32, 32, 16]          --\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvSandwich: 2-8                 [16, 16, 32, 16]          --\n",
       "│    │    └─Conv2d: 3-22                 [16, 16, 32, 16]          9,216\n",
       "│    │    └─BatchNorm2d: 3-23            [16, 16, 32, 16]          32\n",
       "│    │    └─LeakyReLU: 3-24              [16, 16, 32, 16]          --\n",
       "├─Upsample: 1-15                         [16, 16, 64, 32]          --\n",
       "├─ModuleList: 1-16                       --                        (recursive)\n",
       "│    └─ConvSandwich: 2-9                 [16, 8, 64, 32]           --\n",
       "│    │    └─Conv2d: 3-25                 [16, 8, 64, 32]           2,304\n",
       "│    │    └─BatchNorm2d: 3-26            [16, 8, 64, 32]           16\n",
       "│    │    └─LeakyReLU: 3-27              [16, 8, 64, 32]           --\n",
       "├─Upsample: 1-17                         [16, 8, 128, 64]          --\n",
       "├─Conv2d: 1-18                           [16, 35, 64, 32]          6,020\n",
       "==========================================================================================\n",
       "Total params: 106,940\n",
       "Trainable params: 106,940\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 515.78\n",
       "==========================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 35.13\n",
       "Params size (MB): 0.43\n",
       "Estimated Total Size (MB): 35.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(\n",
    "        hparams[\"batch_size\"],\n",
    "        3,\n",
    "        hparams[\"input_size\"][0],\n",
    "        hparams[\"input_size\"][1],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging CUDA\n",
    "\n",
    "# print(torch.cuda.memory_allocated() / 1000**2)\n",
    "# print(torch.cuda.memory_reserved() / 1024**2)\n",
    "\n",
    "# torch.cuda.memory._dump_snapshot(\"debug/pretraining_memory_usage.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable), total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, loss_func, tb_logger, epochs=3, name=\"segmentation\"\n",
    "):\n",
    "    optimizer = model.optimizer\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7\n",
    "    )\n",
    "\n",
    "    validation_loss = 0\n",
    "\n",
    "    patience = model.hp[\"max_patience\"]\n",
    "    best_val_loss = 10e10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        training_loop = create_tqdm_bar(\n",
    "            train_loader, desc=f\"Training Epoch [{epoch + 1}/{epochs}]\"\n",
    "        )\n",
    "        training_loss = 0\n",
    "        for train_iter, batch in training_loop:\n",
    "            loss = model.training_step(batch, loss_func)\n",
    "            training_loss += loss.item()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            training_loop.set_postfix(\n",
    "                train_loss=\"{:.8f}\".format(training_loss / (train_iter + 1)),\n",
    "                lr=\"{:.8f}\".format(optimizer.param_groups[0][\"lr\"]),\n",
    "            )\n",
    "            tb_logger.add_scalar(\n",
    "                f\"{name}/train_loss\",\n",
    "                loss.item(),\n",
    "                epoch * len(train_loader) + train_iter,\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        val_loop = create_tqdm_bar(\n",
    "            val_loader, desc=f\"Validation Epoch [{epoch + 1}/{epochs}]\"\n",
    "        )\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():  # Don't actually need because validation_step already has it?\n",
    "            for val_iter, batch in val_loop:\n",
    "                loss = model.validation_step(batch, loss_func)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar\n",
    "                val_loop.set_postfix(\n",
    "                    patience=\"{}\".format(patience),\n",
    "                    val_loss=\"{:.8f}\".format(validation_loss / (val_iter + 1)),\n",
    "                )\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(\n",
    "                    f\"{name}/val_loss\",\n",
    "                    validation_loss / (val_iter + 1),\n",
    "                    epoch * len(val_loader) + val_iter,\n",
    "                )\n",
    "\n",
    "        scaled_loss = validation_loss / (\n",
    "            val_iter + 1\n",
    "        )  # validation_loss is sum of batch losses, we want average\n",
    "        if scaled_loss <= best_val_loss:\n",
    "            patience = model.hp[\"max_patience\"]\n",
    "            best_val_loss = scaled_loss  # Rescaled based on batch size\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        # End program if patience is done\n",
    "        if patience == 0:\n",
    "            print(f\"Stopping early at epoch {epoch}! (patience done)\")\n",
    "            print(f\"Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}\")\n",
    "            break\n",
    "\n",
    "        # This value is for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "    print(f\"Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to use the dataset object directly during training, as this is slow. Instead, PyTorch provides a DataLoader class that speeds stuff up when we use batches of data. It even shuffles our training data for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/2]:   0%|                                                                                                   | 0/186 [00:00<?, ?it/s]/home/timm-ubuntu/anaconda3/envs/deep/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training Epoch [1/2]:  99%|██████████████████████████████████████████████████▋| 185/186 [04:28<00:01,  1.44s/it, lr=0.00100000, train_loss=1.46248116]/home/timm-ubuntu/anaconda3/envs/deep/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608847532/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Training Epoch [1/2]: 100%|███████████████████████████████████████████████████| 186/186 [04:30<00:00,  1.45s/it, lr=0.00100000, train_loss=1.46007181]\n",
      "Validation Epoch [1/2]:  56%|███████████████████████████████▌                        | 18/32 [00:26<00:20,  1.46s/it, patience=3, val_loss=1.12076381]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m epochs \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     19\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m train_model(\n\u001b[1;32m     21\u001b[0m     model,\n\u001b[1;32m     22\u001b[0m     train_loader,\n\u001b[1;32m     23\u001b[0m     val_loader,\n\u001b[1;32m     24\u001b[0m     loss_func,\n\u001b[1;32m     25\u001b[0m     tb_logger,\n\u001b[1;32m     26\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     27\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
      "Cell \u001b[0;32mIn[34], line 52\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss_func, tb_logger, epochs, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Don't actually need because validation_step already has it?\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_iter, batch \u001b[38;5;129;01min\u001b[39;00m val_loop:\n\u001b[1;32m     53\u001b[0m         loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvalidation_step(batch, loss_func)\n\u001b[1;32m     54\u001b[0m         validation_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[24], line 125\u001b[0m, in \u001b[0;36mFixedCityScapes.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(path) \u001b[38;5;28;01mas\u001b[39;00m im:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Image has to be converted to RGB explictly\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m         im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m         im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mresize(\n\u001b[1;32m    127\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaled_h), Resampling\u001b[38;5;241m.\u001b[39mNEAREST\n\u001b[1;32m    128\u001b[0m         )\n\u001b[1;32m    129\u001b[0m         image \u001b[38;5;241m=\u001b[39m to_tensor(im)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.12/site-packages/PIL/Image.py:941\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     colors: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    943\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep/lib/python3.12/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "model = model.to(\n",
    "    device\n",
    ")  # We move the model to the GPU if we can (this is very easy to forget to do!!!)\n",
    "\n",
    "# Create the tb_logger\n",
    "path = os.path.join(\"logs\", \"segmentation_logs\")\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f\"run_{num_of_runs + 1}\")\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False)\n",
    "\n",
    "epochs = hparams.get(\"epochs\", 5)\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\")\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    loss_func,\n",
    "    tb_logger,\n",
    "    epochs=epochs,\n",
    "    name=\"segmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Save and load model. Saving and loading not part of the class because it makes pickling unreliable\n",
    "\n",
    "\n",
    "def save(model: nn.Module, path, file_name):\n",
    "    print(f\"Saving the model to '{path}' folder\")\n",
    "    model.cpu()  # First move the model back to the CPU. Note it is no longer on the GPU, so no CUDA!\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model_path = os.path.join(\n",
    "        path, file_name + \".pt\"\n",
    "    )  # File ending .pt for 'PyTorch' model\n",
    "    torch.save(model, model_path)\n",
    "    print(next(model.parameters()).is_cuda)\n",
    "    model.to(device)  # Move model back to main device\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    model: nn.Module = torch.load(path)\n",
    "    model.eval()\n",
    "    print(f\"Loading the model at '{path}'\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# If you don't want to redo training the model, you can simply load a model here and run the rest of the script\n",
    "save(model, path=\"models\", file_name=\"segmentation_unet\")\n",
    "\n",
    "# model = load(\"models/segmentation_unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def visualizer(model, device, test_data):\n",
    "    n = len(test_data)\n",
    "    plt.figure(figsize=(20, 6 * n))\n",
    "\n",
    "    for i, (img, target) in enumerate(test_data):\n",
    "        # Colored input image\n",
    "        plt.subplot(n, 3, i * 3 + 1)\n",
    "        plt.imshow(img.numpy().transpose(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Input image\")\n",
    "\n",
    "        # Prediction\n",
    "        inputs = img.unsqueeze(0)\n",
    "        # inputs = inputs.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        plt.subplot(n, 3, i * 3 + 2)\n",
    "        plt.imshow(pred.numpy().squeeze())\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Prediction (IDs)\")\n",
    "\n",
    "        # Ground truth\n",
    "        plt.subplot(n, 3, i * 3 + 3)\n",
    "        plt.imshow(target[0].squeeze())\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Ground Truth (IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# To be able to visualize the models output with matplotlib, we must load the model back onto the CPU.\n",
    "# The print statement should say false (TODO: make another helper function)\n",
    "\n",
    "model.cpu()\n",
    "print(next(model.parameters()).is_cuda)\n",
    "\n",
    "num_example_imgs = 4\n",
    "visualizer(model, device, train_dataset[6 : 6 + num_example_imgs])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# TODO: calculate how many of the pixels were correctly labeled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
