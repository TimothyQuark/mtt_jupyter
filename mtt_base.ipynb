{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Implement ConvNet model\n",
    "- Implement loading of CIFAR10 dataset\n",
    "- Write training scheme for the buffer\n",
    "- Write training scheme for the distillation\n",
    "- When halfway through training epochs, cut lr by half like in MTT paper\n",
    "- The giant bottleneck in this model is the time wasted on `__getitem__`: applying transforms and endless calls to this function takes up 72% (!!!) of all time spent during training, which is obscene (in comparison, it takes 20ms for a single batch forward pass thanks to CUDA, or 5000/256 * 20 = 390ms for one epoch, compared to about 6s doing transforms). For normalization, this is something that can easily be done on the dataset during initialization, and have to check, but I think MTT doesn't do random transforms doing training of the expert trajectories. It may be worth just doing something like in the MTT repo, lines 38-60 in buffer.py, they seem to have had the same problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Distillation by Matching Training Trajectories\n",
    "\n",
    "Implementation of the MTT method, based on the [code by the paper's authors](https://github.com/GeorgeCazenavette/mtt-distillation/tree/main). This implementation uses Jupyter notebooks to visualize the model, and is also not dependent on any online services like in the original implementation.\n",
    "\n",
    "- Model used: ConvNet (as implemented by the MTT paper)\n",
    "- Dataset used: CIFAR10, CIFAR100\n",
    "\n",
    "To get started, you can run the notebook as is, or modify settings the User Settings section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cells will import important packages, configure devices, load TensorBoard etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Any, Callable, Tuple, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.train_experts import print_line, init_logger, set_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = set_device()\n",
    "print(f\"device being used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Matplotlib so that it properly visualizes inside Jupyter notebook using VSCode\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some small fixes to prevent Jupyter from bugging out\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To prevent the kernel from dying.\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ae6e317668db4e06\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ae6e317668db4e06\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up TensorBoard\n",
    "\n",
    "\"\"\" Notes on superbuggy VSCode tensorboard\n",
    "\n",
    "The general conclusion is that Tensorboard in VSCode breaks regularly for whatever reasons, and nobody\n",
    "cares to fix it. So just use Tensboard from the browser. Code below will run tensorboard on port 6006\n",
    "\n",
    "For some reason tensorboard isn't quitting properly, and keeps the port occupied even\n",
    "if the process itself is killed. Maybe it will be patched someday...\n",
    "\n",
    "1. Check Code Python Interpreter is correct\n",
    "\n",
    "2. Check if port occupied:\n",
    "lsof -i:6006\n",
    "\n",
    "Kill using the PID from above command (not 6006, the PID!):\n",
    "kill PID\n",
    "\n",
    "See https://stackoverflow.com/questions/54395201/tensorboard-could-not-bind-to-port-6006-it-was-already-in-use\n",
    "Also, see this on how to kill tensorboard process (but not the problem here): https://stackoverflow.com/questions/36896164/tensorflow-how-to-close-tensorboard-server\n",
    "\n",
    "3. Alternative if you want to see tensorboard in a webbrowser (6006, 8888 etc):\n",
    "tensorboard --logdir=data/ --host localhost --port 6006\n",
    "http//localhost:6006\n",
    "\n",
    "Another possible bug is that installing tensorboard with conda is not visible in Jupyter Notebooks using VSCode. You will have to reinstall it using pip instead\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Settings\n",
    "\n",
    "Here you can define the most important aspects of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "\n",
    "# User settings:\n",
    "settings: dict[str, Any] = {\n",
    "    \"dataset\": \"CIFAR10\",  # [CIFAR10, CIFAR100]\n",
    "    \"model\": \"ConvNet\",  # [ConvNet, ResNet]\n",
    "    \"ZCA\": False,  # [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "hparams: dict[str, Any] = {\n",
    "    # General hyperparameters\n",
    "    \"device\": device,\n",
    "    \"num_workers\": 16,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 256,\n",
    "    \"max_patience\": 3,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"momentum\": 0.0,\n",
    "    # Model specific hyperparameters\n",
    "    \"img_shape\": (3, 32, 32),  # C x H x W\n",
    "    \"out_channels\": 10,  # Number of output channels (e.g. 100 for CIFAR100)\n",
    "    \"net_depth\": 3,  # Number of Conv layers (excluding helper layers and additional linear layer)\n",
    "    \"net_width\": 128,  # Number of kernels in each Conv layer\n",
    "    \"kernel_size\": 3,  # Dimensions of the kernels (H * W)\n",
    "    \"padding\": 1,  # Conv layer padding\n",
    "    \"activation\": nn.ReLU(inplace=True),  # Activation function\n",
    "    \"norm\": nn.GroupNorm(\n",
    "        num_groups=128, num_channels=128, affine=True\n",
    "    ),  # Normalization technique\n",
    "    \"pooling\": nn.AvgPool2d(kernel_size=2, stride=2),  # Pooling method\n",
    "    \"optimizer\": \"SGD\",  # Optimizer to use for training\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare dataset\n",
    "\n",
    "The models used in this notebook are already provided by PyTorch itself, so we can simply download and modify them. However, the PyTorch class not accept slice indexes, so we will have to write our own little version on top of it which fixes the __getitem__ method. That way, we can do something like train_dataset[0:3] and not just train_dataset[1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset (CIFAR10)\n",
      "Files already downloaded and verified\n",
      "Loading testing dataset (CIFAR10)\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "\n",
    "assert train_dataset != None and test_dataset != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Test set size: 10000\n",
      "Image dimensions: torch.Size([3, 32, 32])\n",
      "Target type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Show some basic information about the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize data\n",
    "\n",
    "# TODO: use torch.tensor.view instead of transpose when possible\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show some of the dataset. Note that normalization means the values are no longer in the range of 0-1, so\n",
    "# there will be clipping warnings.\n",
    "\n",
    "ex_imgs = train_dataset[0:3]\n",
    "\n",
    "visualize_imgs(ex_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Create model\n",
    "- Create tensor that uses one hot encoding for ids, not just a flat image\n",
    "- Use a UNet\n",
    "- Create trainer (use dataloader, load to device)\n",
    "- Use a good loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Classes for our model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder and Decoder\n",
    "\n",
    "# \"\"\"\n",
    "# TODO:\n",
    "# - Optimizer only train encoder or decoder\n",
    "# - Make my own model\n",
    "# - Use PyTorch optimizations from their guide\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# class SegmentationNN(nn.Module):\n",
    "#     def __init__(self, hparams):\n",
    "#         super().__init__()\n",
    "#         self.hp = hparams\n",
    "#         self.kernel_factor = self.hp[\n",
    "#             \"kernel_factor\"\n",
    "#         ]  # Reuse this value often, so don't search a Dict\n",
    "\n",
    "#         \"\"\"\"\n",
    "#         General structure of the model:\n",
    "\n",
    "#         - encoder_block downsamples inputs and learns key features. Intermediate outputs are stored\n",
    "#         and used by the decoder block\n",
    "#         - decoder_block: Upsamples the low dimensional outputs of the encoder_block, essentially\n",
    "#         'learns' how things should look, and reconstructs the full segmented image\n",
    "#         - classifier\n",
    "#         \"\"\"\n",
    "#         self.encoder = nn.ModuleList(\n",
    "#             [\n",
    "#                 ConvSandwich(\n",
    "#                     3, self.kernel_factor, 3, 1, 1\n",
    "#                 ),  # RGB image input has 3 channels\n",
    "#                 ConvSandwich(self.kernel_factor, self.kernel_factor * 2, 3, 1, 1),\n",
    "#                 ConvSandwich(self.kernel_factor * 2, self.kernel_factor * 4, 3, 1, 1),\n",
    "#                 ConvSandwich(self.kernel_factor * 4, self.kernel_factor * 4, 3, 1, 1),\n",
    "#                 ConvSandwich(self.kernel_factor * 4, self.kernel_factor * 8, 3, 1, 1),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         self.decoder = nn.ModuleList(\n",
    "#             [\n",
    "#                 ConvSandwich(\n",
    "#                     self.kernel_factor * (8 + 8), self.kernel_factor * 4, 3, 1, 1\n",
    "#                 ),\n",
    "#                 ConvSandwich(\n",
    "#                     self.kernel_factor * (4 + 4), self.kernel_factor * 4, 3, 1, 1\n",
    "#                 ),\n",
    "#                 ConvSandwich(\n",
    "#                     self.kernel_factor * (4 + 4), self.kernel_factor * 2, 3, 1, 1\n",
    "#                 ),\n",
    "#                 ConvSandwich(self.kernel_factor * (2 + 2), self.kernel_factor, 3, 1, 1),\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Conv2d(\n",
    "#             self.kernel_factor * 2 + 3, self.hp[\"output_size\"], kernel_size=3, padding=1\n",
    "#         )\n",
    "\n",
    "#         self.downsample = nn.MaxPool2d(\n",
    "#             kernel_size=2, stride=2\n",
    "#         )  # Layer reused for downsampling\n",
    "#         self.upsample = nn.Upsample(\n",
    "#             scale_factor=2, mode=\"bicubic\"\n",
    "#         )  # Layer reused for upsampling\n",
    "\n",
    "#         # self.print = PrintLayer()  # Used for debugging\n",
    "\n",
    "#         # We want the model to be stored on GPU if possible\n",
    "#         self.device = hparams.get(\n",
    "#             \"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         )\n",
    "#         self.set_optimizer()\n",
    "\n",
    "#         # Apply initial weights\n",
    "#         self.encoder.apply(weights_init)\n",
    "#         self.decoder.apply(weights_init)\n",
    "#         self.classifier.apply(weights_init)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Unet network\n",
    "\n",
    "#         # Note: need to use multiples of 2 or else rounding makes torch.cat try to concat tensors\n",
    "#         # of different sizes\n",
    "\n",
    "#         proc_x = self.encoder[0](x)\n",
    "#         enc_1 = self.encoder[1](proc_x)\n",
    "#         tmp = self.downsample(enc_1)\n",
    "#         enc_2 = self.encoder[2](tmp)\n",
    "#         tmp = self.downsample(enc_2)\n",
    "#         enc_3 = self.encoder[3](tmp)\n",
    "#         tmp = self.downsample(enc_3)\n",
    "#         enc_4 = self.encoder[4](tmp)\n",
    "\n",
    "#         bottleneck = self.downsample(enc_4)\n",
    "\n",
    "#         tmp = self.upsample(bottleneck)\n",
    "#         dec = self.decoder[0](torch.cat([tmp, enc_4], dim=1))\n",
    "#         tmp = self.upsample(dec)\n",
    "#         dec = self.decoder[1](torch.cat([tmp, enc_3], dim=1))\n",
    "#         tmp = self.upsample(dec)\n",
    "#         dec = self.decoder[2](torch.cat([tmp, enc_2], dim=1))\n",
    "#         tmp = self.upsample(dec)\n",
    "#         dec = self.decoder[3](torch.cat([tmp, enc_1], dim=1))\n",
    "#         tmp = self.upsample(dec)\n",
    "\n",
    "#         dec = torch.cat([dec, proc_x], dim=1)\n",
    "#         dec = torch.cat([dec, x], dim=1)\n",
    "#         x = self.classifier(dec)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def set_optimizer(self):\n",
    "#         self.optimizer = None\n",
    "\n",
    "#         self.optimizer = torch.optim.Adam(\n",
    "#             self.parameters(),\n",
    "#             lr=self.hp[\"learning_rate\"],\n",
    "#             weight_decay=self.hp[\"weight_decay\"],\n",
    "#         )\n",
    "\n",
    "#     def training_step(self, batch, loss_func):\n",
    "#         self.train()\n",
    "#         self.optimizer.zero_grad()  # Reset gradient every batch\n",
    "\n",
    "#         # N = batch size, C = channels (3 for RGB), H = image height, W = image width\n",
    "#         images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "#         target = (\n",
    "#             batch[1][0].to(self.device).squeeze()\n",
    "#         )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "#         # Model makes prediction (forward pass)\n",
    "#         pred = self.forward(images)  # N x C x H x W (C=num of CityScape classes)\n",
    "\n",
    "#         # Calculate loss, do backward pass to update weights, optimizer takes step\n",
    "#         # torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\") wants target to be of type long, not float\n",
    "#         loss = loss_func(pred, target)\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, loss_func):\n",
    "#         loss = 0\n",
    "\n",
    "#         # Set model to eval\n",
    "#         self.eval()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "#             target = (\n",
    "#                 batch[1][0].to(self.device).squeeze()\n",
    "#             )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "#             pred = self.forward(images)\n",
    "#             loss = loss_func(pred, target)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# TODO: play around more with parameters to see if there is better solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is just for demonstration purposes, let us make sure the model is not too big. We want to be able to run it on something like a CPU even if we don't have access to a GPU! A parameter size of 25MB seems to be good enough to get this model working on a mid tier desktop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvNet                                  [256, 10]                 --\n",
       "├─Sequential: 1-1                        [256, 128, 4, 4]          --\n",
       "│    └─ConvSandwich: 2-1                 [256, 128, 16, 16]        --\n",
       "│    │    └─Sequential: 3-1              [256, 128, 16, 16]        3,840\n",
       "│    └─ConvSandwich: 2-4                 --                        (recursive)\n",
       "│    │    └─Sequential: 3-4              --                        (recursive)\n",
       "│    └─ConvSandwich: 2-3                 [256, 128, 8, 8]          --\n",
       "│    │    └─Sequential: 3-3              [256, 128, 8, 8]          147,840\n",
       "│    └─ConvSandwich: 2-4                 --                        (recursive)\n",
       "│    │    └─Sequential: 3-4              --                        (recursive)\n",
       "│    └─ConvSandwich: 2-5                 [256, 128, 4, 4]          --\n",
       "│    │    └─Sequential: 3-5              [256, 128, 4, 4]          147,840\n",
       "├─Linear: 1-2                            [256, 10]                 20,490\n",
       "==========================================================================================\n",
       "Total params: 320,266\n",
       "Trainable params: 320,266\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 13.04\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 704.66\n",
       "Params size (MB): 1.28\n",
       "Estimated Total Size (MB): 709.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = ConvNet(hparams=hparams)\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(\n",
    "        model.hp[\"batch_size\"],\n",
    "        model.hp[\"img_shape\"][0],\n",
    "        model.hp[\"img_shape\"][1],\n",
    "        model.hp[\"img_shape\"][2],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging CUDA\n",
    "\n",
    "# print(torch.cuda.memory_allocated() / 1000**2)\n",
    "# print(torch.cuda.memory_reserved() / 1024**2)\n",
    "\n",
    "# torch.cuda.memory._dump_snapshot(\"debug/pretraining_memory_usage.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Train model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to use the dataset object directly during training, as this is slow. Instead, PyTorch provides a DataLoader class that speeds stuff up when we use batches of data. It even shuffles our training data for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/50]: 100%|██████████████████████████████████████████████████| 196/196 [00:09<00:00, 20.25it/s, lr=0.01000000, train_loss=1.73086250]\n",
      "Training Epoch [2/50]:  33%|████████████████▋                                  | 64/196 [00:03<00:06, 20.59it/s, lr=0.01000000, train_loss=1.46048472]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m epochs \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     20\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m train_model(\n\u001b[1;32m     22\u001b[0m     model,\n\u001b[1;32m     23\u001b[0m     train_loader,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m     loss_func,\n\u001b[1;32m     26\u001b[0m     tb_logger,\n\u001b[1;32m     27\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     28\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtt-expert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss_func, tb_logger, epochs, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_iter, batch \u001b[38;5;129;01min\u001b[39;00m training_loop:\n\u001b[0;32m---> 33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining_step(batch, loss_func)\n\u001b[1;32m     34\u001b[0m     training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[13], line 64\u001b[0m, in \u001b[0;36mConvNet.training_step\u001b[0;34m(self, batch, loss_func)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradient every batch\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# N = batch size, C = channels (3 for RGB), H = image height, W = image width\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Input batch, N x C x H x W\u001b[39;00m\n\u001b[1;32m     65\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Model makes prediction (forward pass)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Save and load model. Saving and loading not part of the class because it makes pickling unreliable\n",
    "\n",
    "\n",
    "def save(model: nn.Module, path, file_name):\n",
    "    print(f\"Saving the model to '{path}' folder\")\n",
    "    model.cpu()  # First move the model back to the CPU. Note it is no longer on the GPU, so no CUDA!\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model_path = os.path.join(\n",
    "        path, file_name + \".pt\"\n",
    "    )  # File ending .pt for 'PyTorch' model\n",
    "    torch.save(model, model_path)\n",
    "    print(next(model.parameters()).is_cuda)\n",
    "    model.to(device)  # Move model back to main device\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    model: nn.Module = torch.load(path)\n",
    "    model.eval()\n",
    "    print(f\"Loading the model at '{path}'\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# If you don't want to redo training the model, you can simply load a model here and run the rest of the script\n",
    "save(model, path=\"models\", file_name=\"segmentation_unet\")\n",
    "\n",
    "# model = load(\"models/segmentation_unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def visualizer(model, device, test_data):\n",
    "    n = len(test_data)\n",
    "    plt.figure(figsize=(20, 6 * n))\n",
    "\n",
    "    for i, (img, target) in enumerate(test_data):\n",
    "        # Colored input image\n",
    "        plt.subplot(n, 3, i * 3 + 1)\n",
    "        plt.imshow(img.numpy().transpose(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Input image\")\n",
    "\n",
    "        # Prediction\n",
    "        inputs = img.unsqueeze(0)\n",
    "        # inputs = inputs.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        plt.subplot(n, 3, i * 3 + 2)\n",
    "        plt.imshow(pred.numpy().squeeze())\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Prediction (IDs)\")\n",
    "\n",
    "        # Ground truth\n",
    "        plt.subplot(n, 3, i * 3 + 3)\n",
    "        plt.imshow(target[0].squeeze())\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Ground Truth (IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# To be able to visualize the models output with matplotlib, we must load the model back onto the CPU.\n",
    "# The print statement should say false (TODO: make another helper function)\n",
    "\n",
    "model.cpu()\n",
    "print(next(model.parameters()).is_cuda)\n",
    "\n",
    "num_example_imgs = 4\n",
    "visualizer(model, device, train_dataset[6 : 6 + num_example_imgs])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'dl (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n dl ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# TODO: calculate how many of the pixels were correctly labeled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
