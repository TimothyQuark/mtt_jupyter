{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    " - Batch norm?\n",
    " - Figure out batch size\n",
    " - Clean up the UNet itself haha lol\n",
    " - Rewrite to be able to use Google Colab, test if faster\n",
    " - Save model hparam to a file when running a model, to use for reference\n",
    " - Show prediction image in Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation of car images\n",
    "\n",
    "In this notebook, we will design a semantic segmentation model using the CamVid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cells will import important packages, configure devices, load TensorBoard etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device being used: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device being used: {device}\")\n",
    "\n",
    "# For debugging CUDA, we will record memory history\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.memory._record_memory_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Matplotlib\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some small fixes to prevent Jupyter from bugging out\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# To prevent the kernel from dying.\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18897), started 3:55:33 ago. (Use '!kill 18897' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fa65baaab0eeb7b0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fa65baaab0eeb7b0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up TensorBoard\n",
    "\n",
    "\"\"\" Personal notes on superbuggy VSCode tensorboard\n",
    "\n",
    "The general conclusion I have is that Tensorboard in VSCode breaks regularly for whatever reasons, and nobody\n",
    "cares to fix it. So just use Tensboard from the browser. Code below will run tensorboard on port 6006\n",
    "\n",
    "For some reason tensorboard isn't quitting properly, and keeps the port occupied even\n",
    "if the process itself is killed.\n",
    "\n",
    "1. Check Code Python Interpreter is i2dl\n",
    "\n",
    "2. Check if port occupied:\n",
    "lsof -i:6006\n",
    "\n",
    "Kill using the PID from above command (not 6006, the PID!):\n",
    "kill PID\n",
    "\n",
    "See https://stackoverflow.com/questions/54395201/tensorboard-could-not-bind-to-port-6006-it-was-already-in-use\n",
    "Also, see this on how to kill tensorboard process (but not the problem here): https://stackoverflow.com/questions/36896164/tensorflow-how-to-close-tensorboard-server\n",
    "\n",
    "3. Alternative if I want to see tensorboard in a webbrowser (6006, 8888 etc):\n",
    "tensorboard --logdir=data/ --host localhost --port 6006\n",
    "http//localhost:6006\n",
    "\n",
    "Another possible bug is that installing tensorboard with conda is not visible in Jupyter Notebooks using VSCode. You will have to reinstall it using pip instead\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=logs/ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CityScape Dataset\n",
    "\n",
    "The CityScape dataset will not download automatically with Torchvision. Instead, you will have to download it from their [website](https://www.cityscapes-dataset.com/) by signing up and downloading the *gtFine* and *leftImg8bit* datasets, and placing them in ```root/data/cityscapes``` folder in the root directory of this repository.\n",
    "\n",
    "Sadly, the CityScapes Class in Torchvision is a bit lacking for our use case.\n",
    "- Does not seem to accept slice indexes, so we will have to write our own little version on top of it which fixes the ```__getitem__``` method. That way, we can do something like ```train_dataset[0:3]``` and not just ```train_dataset[1]```\n",
    "- We want to work directly on image tensors, and not a PIL Image object, because we don't need all of the extra stuff that comes with it.\n",
    "- Our target is passed to ```nn.CrossEntropyLoss()``` when calculating the loss, and needs to be of type ```long``` not ```float```\n",
    "\n",
    "Also, applying our model on the full resolution images would be very long, so we will reduce the image resolution for both input and output images\n",
    "\n",
    "Note that when we do the actual training, we do not use the dataset class directly: we use PyTorch's built in Dataloader class, which speeds up batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "# from torchvision.datasets import Cityscapes\n",
    "# from torchvision.transforms.functional import to_tensor\n",
    "# from PIL.Image import Resampling\n",
    "\n",
    "# \"\"\" TODO:\n",
    "# - Type annotations would be really nice here\n",
    "# - Check if we can make things faster, dataloading is always a bottleneck\n",
    "# - One day, should completely rewrite this class so it is actually fast\n",
    "# \"\"\"\n",
    "\n",
    "# # Downsize the input images, as HD images are too large and training would take forever. Use a power of 2, bigger if you want\n",
    "# # training to be faster\n",
    "# SCALING_FACTOR = 2\n",
    "\n",
    "# # The dataset class\n",
    "# class FixedCityScapes(Cityscapes):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         root: str,\n",
    "#         split: str = \"train\",\n",
    "#         mode: str = \"fine\",\n",
    "#         target_type: List[str] | str = \"instance\",\n",
    "#         transform: Callable[..., Any] | None = None,\n",
    "#         target_transform: Callable[..., Any] | None = None,\n",
    "#         transforms: Callable[..., Any] | None = None,\n",
    "#     ) -> None:\n",
    "#         super().__init__(\n",
    "#             root, split, mode, target_type, transform, target_transform, transforms\n",
    "#         )\n",
    "\n",
    "#         self.scaled_w: int = 2048 // SCALING_FACTOR\n",
    "#         self.scaled_h: int = 1024 // SCALING_FACTOR\n",
    "\n",
    "#         assert (\n",
    "#             np.log2(self.scaled_w).is_integer() and np.log2(self.scaled_h).is_integer()\n",
    "#         ), \"SCALING_FACTOR does not result in width and height being powers of 2\"\n",
    "\n",
    "#         # This will convert an RGB image to a tensor with value [0,1], i.e. rescales!\n",
    "#         # self.to_tensor = vision_tf.ToTensor()\n",
    "\n",
    "#     def __getitem__(self, key) -> Tuple[Any, Any]:\n",
    "#         if isinstance(key, slice):\n",
    "#             # This will take the start, stop and step of the slice\n",
    "#             # self[i] will return the below isinstance(key, int) logic\n",
    "#             return [self[i] for i in range(*key.indices(len(self)))]\n",
    "#         elif isinstance(key, int):\n",
    "#             raw = super().__getitem__(key)\n",
    "#             if self.split in (\"train\", \"val\"):\n",
    "#                 # Returns Tuple[Image, Tuple[Segmentation Mask, Colored Mask]]\n",
    "#                 data = (\n",
    "#                     to_tensor(\n",
    "#                         raw[0].resize(\n",
    "#                             (self.scaled_w, self.scaled_h), Resampling.NEAREST\n",
    "#                         )\n",
    "#                     ),\n",
    "#                     (\n",
    "#                         torch.from_numpy(\n",
    "#                             np.array(\n",
    "#                                 raw[1][0].resize(\n",
    "#                                     (self.scaled_w, self.scaled_h), Resampling.NEAREST\n",
    "#                                 ),\n",
    "#                                 dtype=np.int64,\n",
    "#                             )\n",
    "#                         ),\n",
    "#                         torch.from_numpy(\n",
    "#                             np.array(\n",
    "#                                 raw[1][1].resize(\n",
    "#                                     (self.scaled_w, self.scaled_h), Resampling.NEAREST\n",
    "#                                 ),\n",
    "#                                 dtype=np.int64,\n",
    "#                             )\n",
    "#                         ),\n",
    "#                     ),\n",
    "#                 )\n",
    "#             elif self.split == \"test\":\n",
    "#                 # Returns Tuple[Image, Empty Segmentation Mask]\n",
    "#                 raw = super().__getitem__(key)\n",
    "#                 data = (\n",
    "#                     to_tensor(\n",
    "#                         raw[0].resize(\n",
    "#                             (self.scaled_w, self.scaled_h), Resampling.NEAREST\n",
    "#                         )\n",
    "#                     ),\n",
    "#                     torch.from_numpy(\n",
    "#                         np.array(\n",
    "#                             raw[1].resize(\n",
    "#                                 (self.scaled_w, self.scaled_h), Resampling.NEAREST\n",
    "#                             ),\n",
    "#                             dtype=np.int64,\n",
    "#                         )\n",
    "#                     ),  # This is empty image and pretty useless\n",
    "#                 )\n",
    "#             else:\n",
    "#                 raise KeyError(f\"Type of split is not known: {self.split}\")\n",
    "#             return data\n",
    "#         else:\n",
    "#             raise TypeError(f\"Invalid argument type (key is of type {type(key)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.Image import Resampling\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import verify_str_arg\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "\n",
    "# # Downsize the input images, as HD images are too large and training would take forever. Use a power of 2, bigger if you want\n",
    "# # training to be faster\n",
    "SCALING_FACTOR = 2\n",
    "\n",
    "\n",
    "# Fixed Cityscapes Dataset. It is not as comprehensive as the one provided in torchvision.datasets.Cityscapes, but\n",
    "# is tailored to our needs in this project. Also, it works with CUDA and accepts list slices :)\n",
    "class FixedCityScapes(VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str = \"train\",\n",
    "        target_type: Union[List[str], str] = \"semantic\",\n",
    "        transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "\n",
    "        self.images_dir: str = os.path.join(self.root, \"leftImg8bit\", split)\n",
    "        self.targets_dir: str = os.path.join(self.root, \"gtFine\", split)\n",
    "        self.split: str = split\n",
    "        self.target_type: Tuple[List[str], str] = target_type\n",
    "        self.filepaths: np.ndarray = None\n",
    "\n",
    "        self.scaled_w: int = 2048 // SCALING_FACTOR\n",
    "        self.scaled_h: int = 1024 // SCALING_FACTOR\n",
    "\n",
    "        assert (\n",
    "            np.log2(self.scaled_w).is_integer() and np.log2(self.scaled_h).is_integer()\n",
    "        ), \"SCALING_FACTOR does not result in width and height being powers of 2\"\n",
    "\n",
    "        # Test split is valid\n",
    "        valid_splits = (\"train\", \"val\", \"test\")\n",
    "        if split not in valid_splits:\n",
    "            msg = f\"Split provided not valid: provided {split}, but only {valid_splits} valid\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        # Test target is valid and convert to tuple\n",
    "        if isinstance(target_type, Tuple):\n",
    "            pass\n",
    "        elif isinstance(target_type, List):\n",
    "            self.target_type = tuple(target_type)\n",
    "        elif isinstance(target_type, str):\n",
    "            self.target_type = (target_type,)\n",
    "        else:\n",
    "            msg = f\"Target type not valid: provided {type(target_type)}, but must be of type str or tuple/list of str\"\n",
    "            raise ValueError(msg)\n",
    "        # print(self.filepaths.shape)\n",
    "        # print(type(self.filepaths))\n",
    "        # print(type(self.target_type))\n",
    "\n",
    "        valid_targets = (\"semantic\", \"color\")\n",
    "        if not set(self.target_type).issubset(valid_targets):\n",
    "            msg = f\"Target keys provided not valid: provided {self.target_type}, but only {valid_targets} valid\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        # Dumb way of counting how many images, and then creating array of correct dimensions\n",
    "        num_img = 0\n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                num_img += 1\n",
    "        # print(num_img)\n",
    "        self.filepaths = np.empty(\n",
    "            [num_img, 1 + len(self.target_type)], dtype=object\n",
    "        )  # yes we create an array of type object, not efficient\n",
    "        print(self.filepaths.shape)\n",
    "\n",
    "        # Populate the filepaths for images and targets\n",
    "        idx = 0\n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                target_types = []\n",
    "                for t in self.target_type:\n",
    "                    target_name = \"{}_{}\".format(\n",
    "                        file_name.split(\"_leftImg8bit\")[0],\n",
    "                        self._get_target_suffix(\"gtFine\", t),\n",
    "                    )\n",
    "                    target_types.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "                row = np.array(\n",
    "                    [os.path.join(img_dir, file_name)] + target_types, dtype=object\n",
    "                )\n",
    "                # print(row.shape)\n",
    "                # print(row)\n",
    "                self.filepaths[idx, :] = row\n",
    "                idx += 1\n",
    "\n",
    "    def __getitem__(self, key) -> Tuple[Any, Any]:\n",
    "        image = None # RGB tensor of image of dim Height * Width * Channels\n",
    "        target = np.empty([self.scaled_h, self.scaled_w, self.filepaths.shape[1] - 1], dtype=np.int64) # Tensor of dim Height * Width * Number of targets\n",
    "\n",
    "        for idx, path in enumerate(self.filepaths[key]):\n",
    "            with Image.open(path) as im:\n",
    "                if idx == 0: # Image has to be converted to RGB explictly\n",
    "                    im = im.convert(\"RGB\")\n",
    "                    im = im.resize((self.scaled_w, self.scaled_h), Resampling.NEAREST)\n",
    "                    image = to_tensor(im)\n",
    "                else: # Pixels of target are classifier\n",
    "                    pass\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.filepaths.shape[0]\n",
    "\n",
    "    def _get_target_suffix(self, mode: str, target_type: str) -> str:\n",
    "        if target_type == \"instance\":\n",
    "            return f\"{mode}_instanceIds.png\"\n",
    "        elif target_type == \"semantic\":\n",
    "            return f\"{mode}_labelIds.png\"\n",
    "        elif target_type == \"color\":\n",
    "            return f\"{mode}_color.png\"\n",
    "        else:\n",
    "            return f\"{mode}_polygons.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset\n",
      "(2975, 3)\n",
      "Loading validation dataset\n",
      "(500, 3)\n",
      "Loading test dataset\n",
      "(1525, 2)\n",
      "Done loading datasets\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import Cityscapes\n",
    "\n",
    "# https://pytorch.org/vision/0.15/generated/torchvision.datasets.Cityscapes.html#torchvision.datasets.Cityscapes\n",
    "# https://stackoverflow.com/questions/56650201/how-to-convert-35-classes-of-cityscapes-dataset-to-19-classes\n",
    "# https://www.cityscapes-dataset.com/downloads/\n",
    "\n",
    "# TODO: Augmentations and transforms\n",
    "\n",
    "print(\"Loading training dataset\")\n",
    "train_dataset = FixedCityScapes(\n",
    "    root=\"data/cityscapes\",\n",
    "    split=\"train\",\n",
    "    target_type=[\"semantic\", \"color\"],\n",
    ")\n",
    "print(\"Loading validation dataset\")\n",
    "val_dataset = FixedCityScapes(\n",
    "    root=\"data/cityscapes\", split=\"val\", target_type=[\"semantic\", \"color\"]\n",
    ")\n",
    "print(\"Loading test dataset\")\n",
    "test_dataset = FixedCityScapes(\n",
    "    root=\"data/cityscapes\", split=\"test\", target_type=\"semantic\"\n",
    ")  # Test data is unlabeled, so semantic target is simply black\n",
    "print(\"Done loading datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2975\n",
      "Validation set size: 500\n",
      "Test set size: 1525\n",
      "Image dimensions: torch.Size([512, 1024])\n",
      "Target mask dimensions: (1024, 2)\n",
      "Target tuple size: 512\n"
     ]
    }
   ],
   "source": [
    "# Show some basic information about the dataset\n",
    "\n",
    "print(\"Training set size: %i\" % len(train_dataset))\n",
    "print(\"Validation set size: %i\" % len(val_dataset))\n",
    "print(\"Test set size: %i\" % len(test_dataset))\n",
    "\n",
    "# Dataset returns a tuple of PIL Images, input image and segmented groundtruth\n",
    "# https://pillow.readthedocs.io/en/stable/reference/Image.html#image-attributes\n",
    "first_image, first_target = train_dataset[0]\n",
    "print(f\"Image dimensions: {first_image[0].shape}\")\n",
    "print(f\"Target dimensions: {first_target[0].shape}\")\n",
    "print(\n",
    "    f\"Target tuple size: {len(first_image[1])}\"\n",
    ")  # items called from the dataset are a tuple, first item is input image, second is another tuple with targets\n",
    "\n",
    "# test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize data\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def visualize_imgs(images):\n",
    "    if isinstance(images[0], Tensor):\n",
    "        plt.figure(figsize=(20, 6))\n",
    "\n",
    "        # Input image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[0].numpy().transpose(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Input image\")\n",
    "\n",
    "        # Colored Ground truth segmented image\n",
    "        # Input image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(images[1][1].numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Ground Truth (colored)\")\n",
    "\n",
    "        # Ground truth using ids (so basically shades of grey)\n",
    "        # Input image\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(images[1][0].numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Ground Truth (ids)\")\n",
    "    else:\n",
    "        n = len(images)\n",
    "        plt.figure(figsize=(20, 6 * n))\n",
    "        for i, (img, target) in enumerate(images):\n",
    "            # Input image\n",
    "            plt.subplot(n, 3, i * 3 + 1)\n",
    "            plt.imshow(img.numpy().transpose(1, 2, 0))\n",
    "            plt.axis(\"off\")\n",
    "            if i == 0:\n",
    "                plt.title(\"Input image\")\n",
    "\n",
    "            # Colored Ground truth segmented image\n",
    "            # Input image\n",
    "            plt.subplot(n, 3, i * 3 + 2)\n",
    "            plt.imshow(target[1].numpy())\n",
    "            plt.axis(\"off\")\n",
    "            if i == 0:\n",
    "                plt.title(\"Ground Truth (colored)\")\n",
    "\n",
    "            # Ground truth using ids (so basically shades of grey)\n",
    "            # Input image\n",
    "            plt.subplot(n, 3, i * 3 + 3)\n",
    "            plt.imshow(target[0].numpy())\n",
    "            plt.axis(\"off\")\n",
    "            if i == 0:\n",
    "                plt.title(\"Ground Truth (IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of the dataset\n",
    "\n",
    "num_example_imgs = 4\n",
    "visualize_imgs(train_dataset[0:num_example_imgs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Create model\n",
    "- Create tensor that uses one hot encoding for ids, not just a flat image\n",
    "- Use a UNet\n",
    "- Create trainer (use dataloader, load to device)\n",
    "- Use a good loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Classes\n",
    "\n",
    "\n",
    "class ConvSandwich(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class that creates a Convolution Layer with a BatchNorm Layer and Activation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, bias=False\n",
    "        )  # Because we use Batchnorm, bias is averaged out, so don't use\n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper Class for debugging behaviour of model. Will print out the shape of the layer input,\n",
    "    and input passed directly to output\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights for each layer of model m, based on the type of activation function\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_uniform_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "- Optimizer only train encoder of decoder\n",
    "- Make my own model\n",
    "- Use PyTorch optimizations from their guide:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SegmentationNN(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hp = hparams\n",
    "        self.kernel_factor = self.hp[\n",
    "            \"kernel_factor\"\n",
    "        ]  # Reuse this value often, so don't search a Dict\n",
    "\n",
    "        \"\"\"\"\n",
    "        General structure of the model:\n",
    "\n",
    "        - encoder_block downsamples inputs and learns key features. Intermediate outputs are stored\n",
    "        and used by the decoder block\n",
    "        - decoder_block: Upsamples the low dimensional outputs of the encoder_block, essentially\n",
    "        'learns' how things should look, and reconstructs the full segmented image\n",
    "        - classifier\n",
    "        \"\"\"\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                ConvSandwich(\n",
    "                    3, self.kernel_factor, 3, 1, 1\n",
    "                ),  # RGB image input has 3 channels\n",
    "                ConvSandwich(self.kernel_factor, self.kernel_factor * 2, 3, 1, 1),\n",
    "                ConvSandwich(self.kernel_factor * 2, self.kernel_factor * 4, 3, 1, 1),\n",
    "                ConvSandwich(self.kernel_factor * 4, self.kernel_factor * 4, 3, 1, 1),\n",
    "                ConvSandwich(self.kernel_factor * 4, self.kernel_factor * 8, 3, 1, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            [\n",
    "                ConvSandwich(\n",
    "                    self.kernel_factor * (8 + 8), self.kernel_factor * 4, 3, 1, 1\n",
    "                ),\n",
    "                ConvSandwich(\n",
    "                    self.kernel_factor * (4 + 4), self.kernel_factor * 4, 3, 1, 1\n",
    "                ),\n",
    "                ConvSandwich(\n",
    "                    self.kernel_factor * (4 + 4), self.kernel_factor * 2, 3, 1, 1\n",
    "                ),\n",
    "                ConvSandwich(self.kernel_factor * (2 + 2), self.kernel_factor, 3, 1, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Conv2d(\n",
    "            self.kernel_factor * 2 + 3, self.hp[\"output_size\"], kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # Layer reused for downsampling\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bicubic\"\n",
    "        )  # Layer reused for upsampling\n",
    "\n",
    "        # self.print = PrintLayer()  # Used for debugging\n",
    "\n",
    "        self.device = hparams.get(\n",
    "            \"device\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.set_optimizer()\n",
    "\n",
    "        # Apply initial weights\n",
    "        self.encoder.apply(weights_init)\n",
    "        self.decoder.apply(weights_init)\n",
    "        self.classifier.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Unet network\n",
    "\n",
    "        # Note: need to use multiples of 2 or else rounding makes torch.cat try to concat tensors\n",
    "        # of different sizes\n",
    "\n",
    "        proc_x = self.encoder[0](x)\n",
    "        enc_1 = self.encoder[1](proc_x)\n",
    "        tmp = self.downsample(enc_1)\n",
    "        enc_2 = self.encoder[2](tmp)\n",
    "        tmp = self.downsample(enc_2)\n",
    "        enc_3 = self.encoder[3](tmp)\n",
    "        tmp = self.downsample(enc_3)\n",
    "        enc_4 = self.encoder[4](tmp)\n",
    "\n",
    "        bottleneck = self.downsample(enc_4)\n",
    "\n",
    "        tmp = self.upsample(bottleneck)\n",
    "        dec = self.decoder[0](torch.cat([tmp, enc_4], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "        dec = self.decoder[1](torch.cat([tmp, enc_3], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "        dec = self.decoder[2](torch.cat([tmp, enc_2], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "        dec = self.decoder[3](torch.cat([tmp, enc_1], dim=1))\n",
    "        tmp = self.upsample(dec)\n",
    "\n",
    "        dec = torch.cat([dec, proc_x], dim=1)\n",
    "        dec = torch.cat([dec, x], dim=1)\n",
    "        x = self.classifier(dec)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_optimizer(self):\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hp[\"learning_rate\"],\n",
    "            weight_decay=self.hp[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, loss_func):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()  # Reset gradient every batch\n",
    "\n",
    "        # N = batch size, C = channels (3 for RGB), H = image height, W = image width\n",
    "        images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "        target = (\n",
    "            batch[1][0].to(self.device).squeeze()\n",
    "        )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "        # Model makes prediction (forward pass)\n",
    "        pred = self.forward(images)  # N x C x H x W (C=num of CityScape classes)\n",
    "\n",
    "        # Calculate loss, do backward pass to update weights, optimizer takes step\n",
    "        # torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\") wants target to be of type long, not float\n",
    "        loss = loss_func(pred, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, loss_func):\n",
    "        loss = 0\n",
    "\n",
    "        # Set model to eval\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = batch[0].to(self.device)  # Input batch, N x C x H x W\n",
    "            target = (\n",
    "                batch[1][0].to(self.device).squeeze()\n",
    "            )  # Ground truth, each pixel assigned an ID int. N x H x W\n",
    "\n",
    "            pred = self.forward(images)\n",
    "            loss = loss_func(pred, target)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "hparams = {\n",
    "    \"device\": device,\n",
    "    \"num_workers\": 16,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 8,\n",
    "    \"kernel_factor\": 32,  # Multiplied by powers of 2 to create number of kernels in Conv layers\n",
    "    \"max_patience\": 3,\n",
    "    \"input_size\": np.array((2048, 1024))\n",
    "    // SCALING_FACTOR,  # Scaled image resolution (tuple), variable not used by actual model since CNN. It is defined in a previous cell because we used it for the dataset class\n",
    "    \"output_size\": len(Cityscapes.classes),  # Classes in CityScape, should be 35\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is just for demonstration purposes, let us make sure the model is not too big. We want to be able to run it on something like a CPU even if we don't have access to a GPU! A parameter size of 25MB seems to be good enough to get this model working on a mid tier desktop CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(\n",
    "        hparams[\"batch_size\"],\n",
    "        3,\n",
    "        hparams[\"input_size\"][0],\n",
    "        hparams[\"input_size\"][1],\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging CUDA\n",
    "\n",
    "print(torch.cuda.memory_allocated() / 1000**2)\n",
    "print(torch.cuda.memory_reserved() / 1024**2)\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"debug/pretraining_memory_usage.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable), total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, loss_func, tb_logger, epochs=3, name=\"segmentation\"\n",
    "):\n",
    "    optimizer = model.optimizer\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7\n",
    "    )\n",
    "\n",
    "    validation_loss = 0\n",
    "    model = model.to(device)\n",
    "\n",
    "    patience = model.hp[\"max_patience\"]\n",
    "    best_val_loss = 10e10\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        training_loop = create_tqdm_bar(\n",
    "            train_loader, desc=f\"Training Epoch [{epoch + 1}/{epochs}]\"\n",
    "        )\n",
    "        training_loss = 0\n",
    "        for train_iter, batch in training_loop:\n",
    "            loss = model.training_step(batch, loss_func)\n",
    "            training_loss += loss.item()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            training_loop.set_postfix(\n",
    "                train_loss=\"{:.8f}\".format(training_loss / (train_iter + 1)),\n",
    "                lr=\"{:.8f}\".format(optimizer.param_groups[0][\"lr\"]),\n",
    "            )\n",
    "            tb_logger.add_scalar(\n",
    "                f\"{name}/train_loss\",\n",
    "                loss.item(),\n",
    "                epoch * len(train_loader) + train_iter,\n",
    "            )\n",
    "\n",
    "        # Validation\n",
    "        val_loop = create_tqdm_bar(\n",
    "            val_loader, desc=f\"Validation Epoch [{epoch + 1}/{epochs}]\"\n",
    "        )\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():  # Don't actually need because validation_step already has it?\n",
    "            for val_iter, batch in val_loop:\n",
    "                loss = model.validation_step(batch, loss_func)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar\n",
    "                val_loop.set_postfix(\n",
    "                    patience=\"{}\".format(patience),\n",
    "                    val_loss=\"{:.8f}\".format(validation_loss / (val_iter + 1)),\n",
    "                )\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(\n",
    "                    f\"{name}/val_loss\",\n",
    "                    validation_loss / (val_iter + 1),\n",
    "                    epoch * len(val_loader) + val_iter,\n",
    "                )\n",
    "\n",
    "        scaled_loss = validation_loss / (\n",
    "            val_iter + 1\n",
    "        )  # validation_loss is sum of batch losses, we want average\n",
    "        if scaled_loss <= best_val_loss:\n",
    "            patience = model.hp[\"max_patience\"]\n",
    "            best_val_loss = scaled_loss  # Rescaled based on batch size\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        # End program if patience is done\n",
    "        if patience == 0:\n",
    "            print(f\"Stopping early at epoch {epoch}! (patience done)\")\n",
    "            print(f\"Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}\")\n",
    "            break\n",
    "\n",
    "        # This value is for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)\n",
    "    print(f\"Best Val loss: {best_val_loss}. Final Val loss used: {scaled_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to use the dataset object directly during training, as this is slow. Instead, PyTorch provides a DataLoader class that speeds stuff up when we use batches of data. It even shuffles our training data for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SegmentationNN(hparams=hparams)\n",
    "\n",
    "# Create the tb_logger\n",
    "path = os.path.join(\"logs\", \"segmentation_logs\")\n",
    "num_of_runs = len(os.listdir(path)) if os.path.exists(path) else 0\n",
    "path = os.path.join(path, f\"run_{num_of_runs + 1}\")\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the model\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False)\n",
    "\n",
    "epochs = hparams.get(\"epochs\", 5)\n",
    "loss_func = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"mean\")\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    loss_func,\n",
    "    tb_logger,\n",
    "    epochs=epochs,\n",
    "    name=\"segmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model. Saving and loading not part of the class because it makes pickling unreliable\n",
    "\n",
    "\n",
    "def save(model, path, file_name):\n",
    "    print(f\"Saving the model to '{path}' folder\")\n",
    "    model = model.cpu()\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    model_path = os.path.join(\n",
    "        path, file_name + \".pt\"\n",
    "    )  # File ending .pt for 'PyTorch' model\n",
    "    torch.save(model, model_path)\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    print(f\"Loading the model at '{path}'\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, path=\"models\", file_name=\"segmentation_unet\")\n",
    "\n",
    "# model = load(\"models/segmentation_unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizer(model, device, test_data):\n",
    "    n = len(test_data)\n",
    "    plt.figure(figsize=(20, 6 * n))\n",
    "\n",
    "    for i, (img, target) in enumerate(test_data):\n",
    "        # Colored input image\n",
    "        plt.subplot(n, 3, i * 3 + 1)\n",
    "        plt.imshow(img.numpy().transpose(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Input image\")\n",
    "\n",
    "        # Prediction\n",
    "        inputs = img.unsqueeze(0)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model.forward(inputs)\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        plt.subplot(n, 3, i * 3 + 2)\n",
    "        plt.imshow(pred.numpy().squeeze())\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Prediction (IDs)\")\n",
    "\n",
    "        # Ground truth\n",
    "        plt.subplot(n, 3, i * 3 + 3)\n",
    "        plt.imshow(target[0])\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"Ground Truth (IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_example_imgs = 4\n",
    "visualizer(model, device, train_dataset[6 : num_example_imgs + 6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
