{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to calculate backprop for the grand loss in MTT, i.e. 'gradient of gradients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- Do a few update steps with the same input, and see if the grand loss is decreasing\\n- Manually calculate the forward pass instead of using PyTorch layers (maybe...)\\n- Extend this to ConvNet, compare to whatever the results are when using MTT Reparam hack\\n- PyTorch uses AddmmBackward0 instead of MmBackward0 when using bias, check if this makes a difference\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "\"\"\"\n",
    "- Do a few update steps with the same input, and see if the grand loss is decreasing\n",
    "- Manually calculate the forward pass instead of using PyTorch layers (maybe...)\n",
    "- Extend this to ConvNet, compare to whatever the results are when using MTT Reparam hack\n",
    "- PyTorch uses AddmmBackward0 instead of MmBackward0 when using bias, check if this makes a difference\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5212, grad_fn=<DivBackward0>)\n",
      "tensor([[ 1.5755e-05, -4.8960e-05, -6.8672e-06, -5.2808e-05],\n",
      "        [-2.2360e-07,  3.0951e-05, -6.7710e-07, -4.0566e-05],\n",
      "        [-3.1944e-05, -1.9887e-05, -1.9220e-06,  4.5667e-05],\n",
      "        [ 5.3278e-07,  4.8682e-05,  5.8707e-06,  5.0788e-06],\n",
      "        [-3.2716e-05, -1.9009e-05, -2.0313e-06,  4.8220e-05],\n",
      "        [ 3.3390e-06, -2.7222e-05,  2.4501e-05,  5.1113e-05],\n",
      "        [ 2.6293e-06, -2.7154e-05,  2.4505e-05,  5.1392e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate dw using PyTorch autograd, and then manually by calculating the forward pass using x,\n",
    "# so that we can use autograd to backprop the grand loss, eventually implmenting manually\n",
    "\n",
    "gradients = []\n",
    "\n",
    "# Define hook function to capture incoming gradients\n",
    "def save_gradients(module, grad_input, grad_output):\n",
    "    gradients.append(grad_input)\n",
    "\n",
    "\n",
    "# Input batch\n",
    "N = 7\n",
    "\n",
    "# Linear layer shapes\n",
    "l1 = (4, 6) # first number is x features\n",
    "l2 = (6, 5)\n",
    "l3 = (5, 9)\n",
    "l4 = (9, 12)\n",
    "\n",
    "# PYTORCH CONVENTIONAL SGD STEP, used to calculate dout for different layers\n",
    "\n",
    "# Used for PyTorch, technically can turn off bias if needed\n",
    "layers_truth = nn.Sequential(\n",
    "    nn.Linear(*l1, bias=False),\n",
    "    nn.Linear(*l2, bias=False),\n",
    "    nn.Linear(*l3, bias=False),\n",
    "    nn.Linear(*l4, bias=False),\n",
    ")\n",
    "# layers_truth = nn.Sequential(\n",
    "#     nn.Linear(*l1),\n",
    "#     nn.Linear(*l2),\n",
    "#     nn.Linear(*l3),\n",
    "#     nn.Linear(*l4),\n",
    "# )\n",
    "\n",
    "# Used for grand loss. This deep copy is independent of the one used by PyTorch\n",
    "layers_copy = copy.deepcopy(layers_truth)\n",
    "\n",
    "# Save incoming gradients of PyTorch model Linear Layers\n",
    "for layer in layers_truth:\n",
    "    layer.register_full_backward_hook(save_gradients)\n",
    "\n",
    "x = torch.randn(N, l1[0])  # Does not have a grad until we do grand loss\n",
    "out = layers_truth(x)  # Forward Pass PyTorch\n",
    "out.retain_grad()  # Retain incoming gradient of last Linear layer\n",
    "# Imaginary target values. Must be type torch.long for CE function\n",
    "target = torch.empty(N, dtype=torch.long).random_(\n",
    "    layers_truth[-1].weight.shape[1]\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_sgd = loss_fn(out, target)\n",
    "loss_sgd.backward(retain_graph=True)\n",
    "\n",
    "# Ensure that no x gradients have been calculated so far\n",
    "# or they will interfere with grand loss backprop\n",
    "assert x.grad == None\n",
    "\n",
    "# We only copy dw and not db because it is a function of x\n",
    "# We detach them so that they are not part of a computational graph during autograd\n",
    "# On FPGA, these will simply be stored in memory\n",
    "grad_truth = {\n",
    "    \"l1_dw\": layers_truth[0].weight.grad.detach().clone(),\n",
    "    \"l2_dw\": layers_truth[1].weight.grad.detach().clone(),\n",
    "    \"l3_dw\": layers_truth[2].weight.grad.detach().clone(),\n",
    "    \"l4_dw\": layers_truth[3].weight.grad.detach().clone(),\n",
    "    \"l4_dout\": out.grad.detach().clone(),  # Special case\n",
    "    \"l3_dout\": gradients[0][0].detach().clone(),\n",
    "    \"l2_dout\": gradients[1][0].detach().clone(),\n",
    "    \"l1_dout\": gradients[2][0].detach().clone(),\n",
    "}\n",
    "\n",
    "# Now let us perform SGD using built in PyTorch functionality\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     [l.weight for l in layers_truth] + [l.bias for l in layers_truth], lr=0.1\n",
    "# )\n",
    "optimizer = torch.optim.SGD([l.weight for l in layers_truth], lr=0.1) # No bias\n",
    "optimizer.step()\n",
    "\n",
    "# Grad truths are supposed to be constants, because douts are not functions of x\n",
    "# and the dw are only used for verification\n",
    "for key, val in grad_truth.items():\n",
    "    assert val.is_leaf, f\"{key} is not a leaf, but should be\"\n",
    "    # print(f\"{key} is leaf: {val.is_leaf}\")\n",
    "\n",
    "# END OF PYTORCH TRUTH\n",
    "\n",
    "# Now we begin to calculate backprop for grand loss\n",
    "\n",
    "x.requires_grad_(True)  # Now we want to calculate gradient of x\n",
    "\n",
    "\"\"\"\n",
    "This is a very important step for making life easier:\n",
    "- We only use the weight tensors, we don't need to backprop\n",
    "to update their values (i.e. compute gradients)\n",
    "- Hence we explicitly don't compute their gradients during back prop,\n",
    "this makes the visualization graph A LOT easier to understand\n",
    "\"\"\"\n",
    "for idx, l in enumerate(layers_copy):\n",
    "    assert l.weight.grad == None\n",
    "    l.weight.requires_grad_(False)\n",
    "    # l.bias.requires_grad_(False)\n",
    "    # Check layer copy is not same as layer truth after SGD\n",
    "    assert not torch.allclose(l.weight, layers_truth[idx].weight)\n",
    "\n",
    "# # Now we manually calculate the dw so we can perform backprop to find x\n",
    "# # Remember, for Linear layer, dw = dout.T @ input\n",
    "grad_val = {\n",
    "    \"l1_dw\": grad_truth[\"l1_dout\"].T @ x,\n",
    "    \"l2_dw\": grad_truth[\"l2_dout\"].T @ layers_copy[0:1](x),\n",
    "    \"l3_dw\": grad_truth[\"l3_dout\"].T @ layers_copy[0:2](x),\n",
    "    \"l4_dw\": grad_truth[\"l4_dout\"].T @ layers_copy[0:3](x),\n",
    "}  # MmBackward0\n",
    "\n",
    "# Ensure that all of these gradients require gradient calculations\n",
    "for grad in grad_val.values():\n",
    "    assert grad.requires_grad\n",
    "    # print(grad.requires_grad)\n",
    "\n",
    "# Ensure calculated gradients match PyTorch\n",
    "assert torch.allclose(grad_val[\"l1_dw\"], grad_truth[\"l1_dw\"])\n",
    "assert torch.allclose(grad_val[\"l2_dw\"], grad_truth[\"l2_dw\"])\n",
    "assert torch.allclose(grad_val[\"l3_dw\"], grad_truth[\"l3_dw\"])\n",
    "assert torch.allclose(grad_val[\"l4_dw\"], grad_truth[\"l4_dw\"])\n",
    "\n",
    "# No dw are leaves, because they are all functions of x\n",
    "for key, val in grad_val.items():\n",
    "    assert not val.is_leaf\n",
    "    # print(f\"{key} is leaf: {val.is_leaf}\")\n",
    "\n",
    "# # # # Tuples for each layer, ignoring bias again since not function of x\n",
    "# # parameters_val = [l.weight.detach().clone() for l in layers]\n",
    "\n",
    "# Now do SGD manually to ensure correct behaviour and compare to PyTorch SGD\n",
    "# These are the updated weight matrices for each Linear layer\n",
    "layers_val = []\n",
    "for idx, layer in enumerate(layers_copy):\n",
    "    # Updated parameters with knowledge of x\n",
    "    # SubBackward0, MulBackward0\n",
    "    layers_val.append(layer.weight - 0.1 * grad_val[f\"l{idx + 1}_dw\"])\n",
    "    assert torch.allclose(layers_val[idx], layers_truth[idx].weight)\n",
    "\n",
    "# Now we flatten all the parameters, to calculate grand loss\n",
    "# CatBackward0, ViewBackward0\n",
    "p_stacked = torch.cat([p.reshape(-1) for p in layers_val]).requires_grad_(True)\n",
    "\n",
    "# Imaginary target values. In MTT, these correspond to the start and end expert parameters\n",
    "exp_end = torch.randn(p_stacked.shape[0])  # Expert end parameters\n",
    "exp_start = torch.randn(p_stacked.shape[0])  # Expert end parameters\n",
    "\n",
    "param_loss_fn = nn.MSELoss(reduction=\"sum\")  # Unnormalized, just like MTT\n",
    "param_dist_fn = nn.MSELoss(reduction=\"sum\")  # Unnormalized, just like MTT\n",
    "\n",
    "param_loss = param_loss_fn(p_stacked, exp_end)  # MseLossBackward0\n",
    "param_loss /= p_stacked.shape[0]\n",
    "param_dist = param_dist_fn(exp_start, exp_end)  # Normalization factor\n",
    "param_dist /= p_stacked.shape[0]\n",
    "\n",
    "loss_grand = param_loss / param_dist\n",
    "\n",
    "assert x.grad == None  # No gradient calculated for x yet\n",
    "loss_grand.backward(retain_graph=True)  # Now we do backprop\n",
    "print(loss_grand)\n",
    "print(x.grad)  # Now x has a gradient, we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the Conventional network. This is not very interesting\n",
    "\n",
    "# graph = make_dot(loss_sgd)\n",
    "# graph.render(\"img/loss_sgd\", format=\"png\") # Img may be cut off, hence save to a file\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"\"\"\n",
    "# Graph is a bit full, but actually the important things are quite simple to see\n",
    "\n",
    "# - x is the network input, the blue box (7,4) (Batch, x features). We are trying to calculate its gradient w.r.t the grand loss\n",
    "# - MmBackward0 is matrix multiplication, it corresponds to the calculations of dw for each layer,\n",
    "# i.e. dw = dout.T @ z, where dout is incoming gradient and z is layer input.\n",
    "# - Depending on the layer the dw is in, dout must pass through other linear layers, hence\n",
    "# there are 1-4 MmBackward0 depending on the dw in question (Check this is correct!)\n",
    "# - SubBackward0 and MulBackward0 are from the manual SGD step we calculated\n",
    "# - ViewBackward0 and CatBackward0 are from the concatenation in the final steps of grand loss\n",
    "# - MSELoss is self explanatory, DivBackward0 is just the normalization done by MTT\n",
    "\n",
    "# A closer look shows that this is just a sum of backpropagations, where each path is simply one layer deeper\n",
    "# In other words, this is very doable with simple DNN layers.\n",
    "# Also, because the dout have been precomputed, this is probably quite fast if we do it all in parallel.\n",
    "# \"\"\"\n",
    "\n",
    "# # Graph the Network for grand loss calculation\n",
    "# # x is blue box with following dimensions:\n",
    "# print(x.shape)\n",
    "\n",
    "# # graph = make_dot(loss_grand, show_attrs=True) # Verbose, requires retaining graph\n",
    "# graph = make_dot(loss_grand)\n",
    "# graph.render(\n",
    "#     \"img/loss_grand\", format=\"png\"\n",
    "# )  # Img may be cut off, hence save to a file\n",
    "# graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let us test this setup for multiple steps. The expectation is that we\n",
    "# # will optimize x such that the grand loss is always decreasing\n",
    "\n",
    "# # Calculate dw using PyTorch autograd, and then manually by calculating the forward pass using x,\n",
    "# # so that we can use autograd to backprop the grand loss, eventually implmenting manually\n",
    "\n",
    "# gradients = []\n",
    "\n",
    "\n",
    "# # Define hook function to capture incoming gradients\n",
    "# def save_gradients(module, grad_input, grad_output):\n",
    "#     gradients.append(grad_input)\n",
    "\n",
    "\n",
    "# # Input batch\n",
    "# N = 7\n",
    "\n",
    "# # Linear layer shapes\n",
    "# l1 = (4, 6) # first number is x features\n",
    "# l2 = (6, 5)\n",
    "# l3 = (5, 9)\n",
    "# l4 = (9, 12)\n",
    "\n",
    "# # PYTORCH CONVENTIONAL SGD STEP, used to calculate dout for different layers\n",
    "\n",
    "# # Used for PyTorch, technically can turn off bias if needed\n",
    "# layers_truth = nn.Sequential(\n",
    "#     nn.Linear(*l1, bias=False),\n",
    "#     nn.Linear(*l2, bias=False),\n",
    "#     nn.Linear(*l3, bias=False),\n",
    "#     nn.Linear(*l4, bias=False),\n",
    "# )\n",
    "# # layers_truth = nn.Sequential(\n",
    "# #     nn.Linear(*l1),\n",
    "# #     nn.Linear(*l2),\n",
    "# #     nn.Linear(*l3),\n",
    "# #     nn.Linear(*l4),\n",
    "# # )\n",
    "\n",
    "# # Used for grand loss. This deep copy is independent of the one used by PyTorch\n",
    "# layers_copy = copy.deepcopy(layers_truth)\n",
    "\n",
    "# # Save incoming gradients of PyTorch model Linear Layers\n",
    "# for layer in layers_truth:\n",
    "#     layer.register_full_backward_hook(save_gradients)\n",
    "\n",
    "# x = torch.randn(N, l1[0])  # Does not have a grad until we do grand loss\n",
    "# optimizer_x = torch.optim.SGD([x], lr=0.1) # No bias\n",
    "# out = layers_truth(x)  # Forward Pass PyTorch\n",
    "# out.retain_grad()  # Retain incoming gradient of last Linear layer\n",
    "# # Imaginary target values. Must be type torch.long for CE function\n",
    "# target = torch.empty(N, dtype=torch.long).random_(\n",
    "#     layers_truth[-1].weight.shape[1]\n",
    "# )\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_sgd = loss_fn(out, target)\n",
    "# loss_sgd.backward(retain_graph=True)\n",
    "\n",
    "# # Ensure that no x gradients have been calculated so far\n",
    "# # or they will interfere with grand loss backprop\n",
    "# assert x.grad == None\n",
    "\n",
    "# # We only copy dw and not db because it is a function of x\n",
    "# # We detach them so that they are not part of a computational graph during autograd\n",
    "# # On FPGA, these will simply be stored in memory\n",
    "# grad_truth = {\n",
    "#     \"l1_dw\": layers_truth[0].weight.grad.detach().clone(),\n",
    "#     \"l2_dw\": layers_truth[1].weight.grad.detach().clone(),\n",
    "#     \"l3_dw\": layers_truth[2].weight.grad.detach().clone(),\n",
    "#     \"l4_dw\": layers_truth[3].weight.grad.detach().clone(),\n",
    "#     \"l4_dout\": out.grad.detach().clone(),  # Special case\n",
    "#     \"l3_dout\": gradients[0][0].detach().clone(),\n",
    "#     \"l2_dout\": gradients[1][0].detach().clone(),\n",
    "#     \"l1_dout\": gradients[2][0].detach().clone(),\n",
    "# }\n",
    "\n",
    "# # Now let us perform SGD using built in PyTorch functionality\n",
    "# # optimizer = torch.optim.SGD(\n",
    "# #     [l.weight for l in layers_truth] + [l.bias for l in layers_truth], lr=0.1\n",
    "# # )\n",
    "# optimizer = torch.optim.SGD([l.weight for l in layers_truth], lr=0.1) # No bias\n",
    "# optimizer.step()\n",
    "\n",
    "# # Grad truths are supposed to be constants, because douts are not functions of x\n",
    "# # and the dw are only used for verification\n",
    "# for key, val in grad_truth.items():\n",
    "#     assert val.is_leaf, f\"{key} is not a leaf, but should be\"\n",
    "#     # print(f\"{key} is leaf: {val.is_leaf}\")\n",
    "\n",
    "# # END OF PYTORCH TRUTH\n",
    "\n",
    "# # Now we begin to calculate backprop for grand loss\n",
    "\n",
    "# # Imaginary target values. In MTT, these correspond to the start and end expert parameters\n",
    "# # Num parameters does not include the dout!\n",
    "# num_param = sum(np.prod(p.weight.shape) for p in layers_truth)\n",
    "# exp_end = torch.randn(num_param)  # Expert end parameters\n",
    "# exp_start = torch.randn(num_param)  # Expert end parameters\n",
    "\n",
    "# steps = 1000\n",
    "\n",
    "# for step in range(steps):\n",
    "\n",
    "#     optimizer_x.zero_grad()\n",
    "\n",
    "#     x.requires_grad_(True)  # Now we want to calculate gradient of x\n",
    "\n",
    "#     \"\"\"\n",
    "#     This is a very important step for making life easier:\n",
    "#     - We only use the weight tensors, we don't need to backprop\n",
    "#     to update their values (i.e. compute gradients)\n",
    "#     - Hence we explicitly don't compute their gradients during back prop,\n",
    "#     this makes the visualization graph A LOT easier to understand\n",
    "#     \"\"\"\n",
    "#     for idx, l in enumerate(layers_copy):\n",
    "#         assert l.weight.grad == None\n",
    "#         l.weight.requires_grad_(False)\n",
    "#         # l.bias.requires_grad_(False)\n",
    "#         # Check layer copy is not same as layer truth after SGD\n",
    "#         assert not torch.allclose(l.weight, layers_truth[idx].weight)\n",
    "\n",
    "#     # # Now we manually calculate the dw so we can perform backprop to find x\n",
    "#     # # Remember, for Linear layer, dw = dout.T @ input\n",
    "#     grad_val = {\n",
    "#         \"l1_dw\": grad_truth[\"l1_dout\"].T @ x,\n",
    "#         \"l2_dw\": grad_truth[\"l2_dout\"].T @ layers_copy[0:1](x),\n",
    "#         \"l3_dw\": grad_truth[\"l3_dout\"].T @ layers_copy[0:2](x),\n",
    "#         \"l4_dw\": grad_truth[\"l4_dout\"].T @ layers_copy[0:3](x),\n",
    "#     }  # MmBackward0\n",
    "\n",
    "#     # Ensure that all of these gradients require gradient calculations\n",
    "#     for grad in grad_val.values():\n",
    "#         assert grad.requires_grad\n",
    "#         # print(grad.requires_grad)\n",
    "\n",
    "#     # Ensure calculated gradients match PyTorch\n",
    "#     # assert torch.allclose(grad_val[\"l1_dw\"], grad_truth[\"l1_dw\"])\n",
    "#     # assert torch.allclose(grad_val[\"l2_dw\"], grad_truth[\"l2_dw\"])\n",
    "#     # assert torch.allclose(grad_val[\"l3_dw\"], grad_truth[\"l3_dw\"])\n",
    "#     # assert torch.allclose(grad_val[\"l4_dw\"], grad_truth[\"l4_dw\"])\n",
    "\n",
    "#     # No dw are leaves, because they are all functions of x\n",
    "#     for key, val in grad_val.items():\n",
    "#         assert not val.is_leaf\n",
    "#         # print(f\"{key} is leaf: {val.is_leaf}\")\n",
    "\n",
    "#     # # # # Tuples for each layer, ignoring bias again since not function of x\n",
    "#     # # parameters_val = [l.weight.detach().clone() for l in layers]\n",
    "\n",
    "#     # Now do SGD manually to ensure correct behaviour and compare to PyTorch SGD\n",
    "#     # These are the updated weight matrices for each Linear layer\n",
    "#     layers_val = []\n",
    "#     for idx, layer in enumerate(layers_copy):\n",
    "#         # Updated parameters with knowledge of x\n",
    "#         # SubBackward0, MulBackward0\n",
    "#         layers_val.append(layer.weight - 0.1 * grad_val[f\"l{idx + 1}_dw\"])\n",
    "#         # assert torch.allclose(layers_val[idx], layers_truth[idx].weight)\n",
    "\n",
    "#     # Now we flatten all the parameters, to calculate grand loss\n",
    "#     # CatBackward0, ViewBackward0\n",
    "#     p_stacked = torch.cat([p.reshape(-1) for p in layers_val]).requires_grad_(True)\n",
    "\n",
    "#     param_loss_fn = nn.MSELoss(reduction=\"sum\")  # Unnormalized, just like MTT\n",
    "#     param_dist_fn = nn.MSELoss(reduction=\"sum\")  # Unnormalized, just like MTT\n",
    "\n",
    "#     param_loss = param_loss_fn(p_stacked, exp_end)  # MseLossBackward0\n",
    "#     param_loss /= p_stacked.shape[0]\n",
    "#     param_dist = param_dist_fn(exp_start, exp_end)  # Normalization factor\n",
    "#     param_dist /= p_stacked.shape[0]\n",
    "\n",
    "#     loss_grand = param_loss / param_dist\n",
    "\n",
    "#     if step == 0:\n",
    "#         assert x.grad == None  # No gradient calculated for x yet\n",
    "#     loss_grand.backward()  # Now we do backprop\n",
    "#     # optimizer_x.step()\n",
    "#     print(loss_grand)\n",
    "#     # print(x.shape)\n",
    "#     # print(x.grad)  # Now x has a gradient, we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are our reference variables that we wish to keep, to compare between models\n",
    "\n",
    "# Input batch\n",
    "N = 7\n",
    "\n",
    "# Linear layer shapes\n",
    "l1 = (4, 6)  # first number is x features\n",
    "l2 = (6, 5)\n",
    "l3 = (5, 9)\n",
    "l4 = (9, 12)\n",
    "\n",
    "class TestLinear(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(*l1, bias=False),\n",
    "            nn.Linear(*l2, bias=False),\n",
    "            nn.Linear(*l3, bias=False),\n",
    "            nn.Linear(*l4, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Don't use _var anywhere except for deepcopy\n",
    "_student_net = TestLinear() # Network to test\n",
    "num_params = sum(np.prod(l.weight.shape) for l in _student_net.layers)\n",
    "_img_syn = torch.randn(N, l1[0]).requires_grad_(True) # Input of network\n",
    "_student_params = torch.cat([l.weight.detach().reshape(-1) for l in _student_net.layers]).requires_grad_(True) # Imaginary, in MTT sometimes called forward_param. These are the weights!\n",
    "starting_params = torch.randn(num_params)  # Imaginary starting expert param\n",
    "target_params = torch.randn(num_params)  # Imaginary ending expert param\n",
    "\n",
    "y_hat = torch.empty(_img_syn.shape[0], dtype=torch.long).random_(_img_syn.shape[1]) # Random targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(0.6175, grad_fn=<DivBackward0>)\n",
      "tensor([[ 2.9508e-05,  5.4805e-06,  7.4864e-05,  2.8809e-05],\n",
      "        [ 3.1661e-05,  3.8795e-06,  7.8810e-05,  2.7912e-05],\n",
      "        [-3.0246e-05, -2.6080e-05,  6.4829e-06, -4.7895e-05],\n",
      "        [ 2.4149e-05, -1.9302e-07, -6.2549e-07, -2.2148e-06],\n",
      "        [-1.5969e-05,  5.4960e-06, -6.3376e-05,  1.8074e-05],\n",
      "        [-3.2719e-05, -2.8099e-05,  6.8503e-06, -5.0283e-05],\n",
      "        [-1.6889e-05,  6.8755e-06, -6.5960e-05,  1.8199e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Let us try to do this now with Reparam from MTT\n",
    "# Note: seem to getting similar garbage as my own implementation, likely because of random inputs\n",
    "\n",
    "from typing import Any\n",
    "from src.reparam_module import ReparamModule\n",
    "from src.networks import ConvNet\n",
    "from src.util import set_device\n",
    "\n",
    "img_syn = copy.deepcopy(_img_syn)\n",
    "student_params = copy.deepcopy(_student_params)\n",
    "\n",
    "optimizer_img = torch.optim.SGD([img_syn], lr=0.1, momentum=0.5)\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Used for updating weights, not x!\n",
    "\n",
    "student_net_mtt = copy.deepcopy(_student_net)\n",
    "student_net_mtt = ReparamModule(student_net_mtt)  # MTT black magic\n",
    "\n",
    "student_net_mtt.train()\n",
    "\n",
    "x = student_net_mtt(img_syn, flat_param=student_params)\n",
    "\n",
    "ce_loss = criterion(x, y_hat)\n",
    "\n",
    "grad = torch.autograd.grad(ce_loss, student_params, create_graph=True)[0]\n",
    "student_params = (student_params - 0.1 * grad)\n",
    "\n",
    "param_loss = torch.nn.functional.mse_loss(student_params, target_params, reduction=\"sum\")\n",
    "param_dist = torch.nn.functional.mse_loss(starting_params, target_params, reduction=\"sum\")\n",
    "\n",
    "param_loss /= num_params\n",
    "param_dist /= num_params\n",
    "\n",
    "param_loss /= param_dist\n",
    "\n",
    "grand_loss = param_loss\n",
    "\n",
    "# Before grand loss, img_syn gradient should be zero\n",
    "print(img_syn.grad)\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "grand_loss.backward()\n",
    "\n",
    "print(grand_loss)\n",
    "print(img_syn.grad)\n",
    "\n",
    "# print(img_syn)\n",
    "# optimizer_img.step()\n",
    "# print(img_syn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(0.6175, grad_fn=<DivBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us do our own method without calculating PyTorch Ground Truth, to see if Autograd is\n",
    "# messing us up\n",
    "\n",
    "# Define hook function to capture incoming gradients\n",
    "gradients = []\n",
    "def save_gradients(module, grad_input, grad_output):\n",
    "    gradients.append(grad_input)\n",
    "\n",
    "img_syn = copy.deepcopy(_img_syn).requires_grad_(False)\n",
    "optimizer_img = torch.optim.SGD([img_syn], lr=0.1, momentum=0.5)\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Used for updating weights, not x!\n",
    "\n",
    "student_net = copy.deepcopy(_student_net)\n",
    "num_params = sum(np.prod(l.weight.shape) for l in student_net.layers)\n",
    "\n",
    "student_net.train()\n",
    "\n",
    "# starting_params = torch.randn(num_params)  # Imaginary starting expert param\n",
    "# target_params = torch.randn(num_params)  # Imaginary ending expert param\n",
    "# student_params = torch.randn(num_params).requires_grad_(True)  # Imaginary, in MTT sometimes called forward_param\n",
    "\n",
    "# Save incoming gradients of PyTorch model Linear Layers\n",
    "for layer in student_net.layers:\n",
    "    layer.register_full_backward_hook(save_gradients)\n",
    "\n",
    "out = student_net(img_syn) # Forward pass\n",
    "out.retain_grad()\n",
    "loss = criterion(out, y_hat) # ce_loss in MTT\n",
    "loss.backward() # This will allow us to calculate the parameter gradients manually\n",
    "\n",
    "# for l in student_net.layers:\n",
    "#     l.weight.requires_grad_(True)\n",
    "\n",
    "# Rewrite gradients into a dictionary to make life easier\n",
    "grad_truth = {\n",
    "    \"l4_dout\": out.grad.detach().clone(),  # Special case\n",
    "    \"l3_dout\": gradients[0][0].detach().clone(),\n",
    "    \"l2_dout\": gradients[1][0].detach().clone(),\n",
    "    \"l1_dout\": gradients[2][0].detach().clone(),\n",
    "}\n",
    "\n",
    "dw = [\n",
    "grad_truth[\"l1_dout\"].T @ img_syn,\n",
    "grad_truth[\"l2_dout\"].T @ student_net.layers[0:1](img_syn),\n",
    "grad_truth[\"l3_dout\"].T @ student_net.layers[0:2](img_syn),\n",
    "grad_truth[\"l4_dout\"].T @ student_net.layers[0:3](img_syn),\n",
    "]\n",
    "# Now we do SGD manually for the weights to calculate the parameters gradients manually\n",
    "\n",
    "for idx, l in enumerate(student_net.layers):\n",
    "    # l.weight.requires_grad_(True)\n",
    "    l.weight = nn.Parameter(l.weight - 0.1 * dw[idx], requires_grad=True) # This will overwrite the tensor, including gradients\n",
    "\n",
    "\n",
    "# Now we flatten all the parameters, to calculate grand loss\n",
    "p_stacked = torch.cat([p.weight.reshape(-1) for p in student_net.layers]).requires_grad_(True)\n",
    "\n",
    "# Imaginary target values. In MTT, these correspond to the start and end expert parameters\n",
    "exp_end = starting_params  # Expert end parameters\n",
    "exp_start = target_params  # Expert end parameters\n",
    "\n",
    "param_loss = torch.nn.functional.mse_loss(p_stacked, target_params, reduction=\"sum\")\n",
    "param_dist = torch.nn.functional.mse_loss(starting_params, target_params, reduction=\"sum\")\n",
    "\n",
    "# param_loss = param_loss_fn(p_stacked, exp_end)  # MseLossBackward0\n",
    "param_loss /= p_stacked.shape[0]\n",
    "# param_dist = param_dist_fn(exp_start, exp_end)  # Normalization factor\n",
    "param_dist /= p_stacked.shape[0]\n",
    "\n",
    "loss_grand = param_loss / param_dist\n",
    "\n",
    "# Before grand loss, img_syn gradient should be zero\n",
    "print(img_syn.grad)\n",
    "optimizer_img.zero_grad()\n",
    "\n",
    "loss_grand.backward() # Calculate gradient of img_syn\n",
    "\n",
    "print(loss_grand)\n",
    "print(img_syn.grad)\n",
    "\n",
    "# print(img_syn.grad)\n",
    "# optimizer_img.zero_grad() # Zero the img gradient before grand loss, this is from the original backprop and unneeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First we test using only linear layers\n",
    "\n",
    "# # Linear layers y = xWT + b\n",
    "\n",
    "# # Layer 1\n",
    "# in_dim = (5, 4)\n",
    "# out_dim = 7\n",
    "# x1 = torch.rand(*in_dim, requires_grad=True)\n",
    "# WT1 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b1 = torch.randn(out_dim, requires_grad=True)\n",
    "# y1 = x1 @ WT1 + b1\n",
    "# y1.retain_grad()\n",
    "# # print(y1.shape)\n",
    "\n",
    "# # Layer 2\n",
    "# in_dim = y1.shape\n",
    "# out_dim = 4\n",
    "# WT2 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b2 = torch.randn(out_dim, requires_grad=True)\n",
    "# y2 = y1 @ WT2 + b2\n",
    "# y2.retain_grad()\n",
    "# # print(y2.shape)\n",
    "\n",
    "# # Layer 3\n",
    "# in_dim = y2.shape\n",
    "# out_dim = 3\n",
    "# WT3 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b3 = torch.randn(out_dim, requires_grad=True)\n",
    "# out = y2 @ WT3 + b3\n",
    "# out.retain_grad() # Output of network\n",
    "\n",
    "# # Sanity check\n",
    "# # print(out == (((x1 @ WT1 + b1) @ WT2 + b2)) @ WT3 + b3)\n",
    "\n",
    "# # Loss\n",
    "# target = torch.randn(out.shape)\n",
    "# loss_fn = nn.MSELoss()\n",
    "# output = loss_fn(out, target)\n",
    "# output.retain_grad()\n",
    "# output.backward()\n",
    "\n",
    "# dout_val = 2 * (out - target) / np.prod(out.shape) # derivative of MSELoss\n",
    "# dx1_val = 0\n",
    "\n",
    "# # print(dout_val)\n",
    "# # print(out.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only tensors that are not computed from other tensors are leaves.\n",
    "# This tells us that all weight and bias tensors are leaves initally.\n",
    "# for var in [x1, y2, out, WT1, WT2, WT3, b1, b2, b3]:\n",
    "#     print(var.is_leaf)\n",
    "\n",
    "# print(WT1)\n",
    "# print(WT1.grad)\n",
    "\n",
    "# optimizer = torch.optim.SGD([WT1, WT2, WT3, b1, b2, b3], lr=0.1)\n",
    "# # Perform one step of SGD\n",
    "# optimizer.step()\n",
    "\n",
    "# # WT1 has now been updated, but is still a leaf (no dependencies!)\n",
    "# print(WT1)\n",
    "\n",
    "# p_stacked = torch.cat[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the grand loss as described in MTT\n",
    "\n",
    "# # First flatten and concatenate all the parameters\n",
    "# # parameters = [\n",
    "# #     WT1.detach().clone(),\n",
    "# #     WT2.detach().clone(),\n",
    "# #     WT3.detach().clone(),\n",
    "# #     b1.detach().clone(),\n",
    "# #     b2.detach().clone(),\n",
    "# #     b3.detach().clone(),\n",
    "# # ]\n",
    "# parameters = [\n",
    "#     WT1,\n",
    "#     WT2,\n",
    "#     WT3,\n",
    "#     b1,\n",
    "#     b2,\n",
    "#     b3,\n",
    "# ]\n",
    "\n",
    "# # All the parameters should be leaves because they don't have dependencies\n",
    "# # for var in parameters:\n",
    "# #     print(var.is_leaf)\n",
    "# p_stacked = torch.cat([p.reshape(-1) for p in parameters]).requires_grad_(True)\n",
    "# # p_stacked.retain_grad()\n",
    "# # Sanity check\n",
    "# num_param = sum(np.prod(p.shape) for p in parameters)\n",
    "# print(p_stacked.shape[0] == num_param)\n",
    "\n",
    "# # # Imaginary start and end expert parameters\n",
    "# # exp_start_stacked = torch.randn(p_stacked.shape)\n",
    "# # exp_end_stacked = torch.randn(p_stacked.shape)\n",
    "\n",
    "# # param_loss_mse = nn.MSELoss(reduction='sum')\n",
    "# # param_dist_mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# # param_loss = param_loss_mse(p_stacked, exp_end_stacked)\n",
    "# # param_loss /= num_param\n",
    "# # param_dist = param_dist_mse(exp_start_stacked, exp_end_stacked)\n",
    "# # param_dist /= num_param\n",
    "\n",
    "# # grand_loss = param_loss / param_dist\n",
    "\n",
    "# # print(grand_loss)\n",
    "# # print(WT1.grad)\n",
    "# # print(p_stacked.grad)\n",
    "# # grand_loss.backward()\n",
    "# # print(p_stacked.grad)\n",
    "# # print(WT1.grad)\n",
    "\n",
    "# # Let us compare this to a manual calculation of the gradients, by using the forward pass\n",
    "\n",
    "# # Do for the variable WT1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First we test using only linear layers\n",
    "\n",
    "# # Linear layers y = xWT + b\n",
    "\n",
    "# # Layer 1\n",
    "# in_dim = (9, 4)\n",
    "# out_dim = 7\n",
    "# x1 = torch.rand(*in_dim, requires_grad=True)\n",
    "# WT1 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b1 = torch.randn(out_dim, requires_grad=True)\n",
    "# y1 = x1 @ WT1 + b1\n",
    "# y1.retain_grad()\n",
    "\n",
    "# # Layer 2\n",
    "# in_dim = y1.shape\n",
    "# out_dim = 4\n",
    "# WT2 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b2 = torch.randn(out_dim, requires_grad=True)\n",
    "# y2 = y1 @ WT2 + b2\n",
    "# y2.retain_grad()\n",
    "\n",
    "# # Layer 3\n",
    "# in_dim = y2.shape\n",
    "# out_dim = 3\n",
    "# WT3 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b3 = torch.randn(out_dim, requires_grad=True)\n",
    "# out = y2 @ WT3 + b3\n",
    "# out.retain_grad() # Output of network\n",
    "\n",
    "# # Sanity check\n",
    "# # print(out == (((x1 @ WT1 + b1) @ WT2 + b2)) @ WT3 + b3)\n",
    "\n",
    "# # output.retain_grad()\n",
    "# # output.backward()\n",
    "\n",
    "# # dout_val = 2 * (out - target) / np.prod(out.shape) # derivative of MSELoss\n",
    "# # dx1_val = 0\n",
    "\n",
    "# # # Sanity check\n",
    "# # print(torch.allclose(dout_val, out.grad))\n",
    "\n",
    "# # First flatten and concatenate all the parameters. These can be considered references,\n",
    "# # not copies, so they retain their computational graphs if used.\n",
    "# student_parameters = {\n",
    "#     \"WT1\": WT1,\n",
    "#     \"WT2\" : WT2,\n",
    "#     \"WT3\" : WT3,\n",
    "#     \"b1\" : b1,\n",
    "#     \"b2\" : b2,\n",
    "#     \"b3\" : b3,\n",
    "#     \"x1\": x1,\n",
    "#     # \"y1\" : y1,\n",
    "#     # \"y2\" : y2\n",
    "# }\n",
    "# # All the parameters should be leaves because they don't have dependencies\n",
    "# # for var in student_parameters.values():\n",
    "# #     print(var.is_leaf)\n",
    "# p_stacked = torch.cat([p.reshape(-1) for p in student_parameters.values()]).requires_grad_(True)\n",
    "\n",
    "# # # Sanity check\n",
    "# num_param = sum(np.prod(p.shape) for p in student_parameters.values())\n",
    "# # print(p_stacked.shape[0] == num_param)\n",
    "\n",
    "# # # Calculate the traditional loss, however instead of using backward method, we use autograd manually\n",
    "# # # In this way, the grad attributes of each parameter is not populated, and we can use them\n",
    "# # # to instead compute the grand loss.\n",
    "# target = torch.empty(out.shape[0], dtype=torch.long).random_(\n",
    "#     out.shape[1]\n",
    "# )  # Imaginary target values. Must be type torch.long for CE function\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# output = loss_fn(out, target)\n",
    "# grad = torch.autograd.grad(outputs=output, inputs=student_parameters.values(), create_graph=True)\n",
    "# grad_stacked = torch.cat([p.reshape(-1) for p in grad]).requires_grad_(True)\n",
    "# grad_descent = p_stacked - 0.1 * grad_stacked\n",
    "\n",
    "# # Sanity check (comment out when computing grand loss!)\n",
    "# # output.backward()\n",
    "# # print(parameters[0].grad)\n",
    "# # print(WT1.grad)\n",
    "# # WT1.grad[0] += 1\n",
    "# # print(parameters[0].grad)\n",
    "# # print(WT1.grad)\n",
    "\n",
    "# # Imaginary start and end expert parameters\n",
    "# exp_start_stacked = torch.randn(p_stacked.shape)\n",
    "# exp_end_stacked = torch.randn(p_stacked.shape)\n",
    "\n",
    "# param_loss_mse = nn.MSELoss(reduction='sum')\n",
    "# param_dist_mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# param_loss = param_loss_mse(grad_descent, exp_end_stacked)\n",
    "# param_loss /= num_param\n",
    "# param_dist = param_dist_mse(exp_start_stacked, exp_end_stacked)\n",
    "# param_dist /= num_param\n",
    "\n",
    "# grand_loss = param_loss / param_dist\n",
    "# # grand_loss = param_loss\n",
    "# print(grand_loss)\n",
    "# grand_loss.backward()\n",
    "# _ = 0\n",
    "\n",
    "# # Not sure how the loss back propagates, hence will rewrite using PyTorch layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1.grad)\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# graph = make_dot(grand_loss, params=student_parameters)\n",
    "\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import os\n",
    "\n",
    "# # Batch\n",
    "# N = 7\n",
    "\n",
    "# # Linear layer shapes\n",
    "# l1 = (4, 6)\n",
    "# l2 = (6, 5)\n",
    "# l3 = (5, 9)\n",
    "\n",
    "# layers = nn.Sequential(\n",
    "#     nn.Linear(*l1), nn.Linear(*l2), nn.Linear(*l3)\n",
    "# )\n",
    "\n",
    "# parameters = [\n",
    "#     layers[0].weight,\n",
    "#     layers[1].weight,\n",
    "#     layers[2].weight\n",
    "# ]\n",
    "\n",
    "# # All the parameters should be leaves because they don't have dependencies\n",
    "# # for var in parameters:\n",
    "# #     print(var.is_leaf)\n",
    "# p_stacked = torch.cat([p.reshape(-1) for p in parameters]).requires_grad_(True)\n",
    "\n",
    "# # Calculate the traditional loss, however instead of using backward method, we use autograd manually\n",
    "# # In this way, the grad attributes of each parameter is not populated, and we can use them\n",
    "# # to instead compute the grand loss.\n",
    "# x = torch.randn(N, l1[0])\n",
    "# out = layers(x)\n",
    "# target = torch.empty(N, dtype=torch.long).random_(\n",
    "#     l3[1]\n",
    "# )  # Imaginary target values. Must be type torch.long for CE function\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# output = loss_fn(out, target)\n",
    "# grad = torch.autograd.grad(outputs=output, inputs=parameters, create_graph=True)\n",
    "# grad_stacked = torch.cat([p.reshape(-1) for p in parameters]).requires_grad_(True)\n",
    "# grad_descent = p_stacked - 0.1 * grad_stacked\n",
    "\n",
    "# # Imaginary start and end expert parameters\n",
    "# exp_start_stacked = torch.randn(p_stacked.shape)\n",
    "# exp_end_stacked = torch.randn(p_stacked.shape)\n",
    "\n",
    "# param_loss_mse = nn.MSELoss(reduction='sum')\n",
    "# param_dist_mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# param_loss = param_loss_mse(grad_descent, exp_end_stacked)\n",
    "# param_loss /= num_param\n",
    "# # param_dist = param_dist_mse(exp_start_stacked, exp_end_stacked)\n",
    "# # param_dist /= num_param\n",
    "\n",
    "# # # TensorBoard\n",
    "# # log_path = os.path.join(\"testing\")\n",
    "# # tb_logger = SummaryWriter(log_path)\n",
    "\n",
    "# # tb_logger.add_graph(\n",
    "# #     layers,\n",
    "# #     torch.randn(7, 4)\n",
    "# # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets instead try to do everything with just a single layer, to understand what is going on\n",
    "\n",
    "# # Layer 1\n",
    "# in_dim = (9, 4)\n",
    "# out_dim = 7\n",
    "# x = torch.rand(*in_dim, requires_grad=True)\n",
    "# WT = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "# b = torch.randn(out_dim, requires_grad=True)\n",
    "# out = x @ WT + b\n",
    "# out.retain_grad()\n",
    "\n",
    "# parameters = {\n",
    "#     # \"x\" : x,\n",
    "#     \"WT\" : WT,\n",
    "#     \"b\" : b\n",
    "# }\n",
    "\n",
    "# p_stacked = torch.cat([p.reshape(-1) for p in parameters.values()]).requires_grad_(True)\n",
    "\n",
    "# # # Sanity check\n",
    "# num_param = sum(np.prod(p.shape) for p in parameters.values())\n",
    "# # print(p_stacked.shape[0] == num_param)\n",
    "\n",
    "# target = torch.empty(out.shape[0], dtype=torch.long).random_(\n",
    "#     out.shape[1]\n",
    "# )  # Imaginary target values. Must be type torch.long for CE function\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# output = loss_fn(out, target)\n",
    "# # output.retain_grad()\n",
    "# # output.backward()\n",
    "\n",
    "# # print(output.grad)\n",
    "# grad = torch.autograd.grad(outputs=output, inputs=parameters.values(), create_graph=True)\n",
    "# print(out.grad)\n",
    "# # grad_stacked = torch.cat([p.reshape(-1) for p in grad]).requires_grad_(True)\n",
    "# # grad_descent = p_stacked - 0.1 * grad_stacked\n",
    "\n",
    "# # # Imaginary start and end expert parameters\n",
    "# # exp_start_stacked = torch.randn(p_stacked.shape)\n",
    "# # exp_end_stacked = torch.randn(p_stacked.shape)\n",
    "\n",
    "# # param_loss_mse = nn.MSELoss(reduction='sum')\n",
    "# # param_dist_mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# # param_loss = param_loss_mse(grad_descent, exp_end_stacked)\n",
    "# # param_loss /= num_param\n",
    "# # param_dist = param_dist_mse(exp_start_stacked, exp_end_stacked)\n",
    "# # param_dist /= num_param\n",
    "\n",
    "# # grand_loss = param_loss / param_dist\n",
    "# # # grand_loss = param_loss\n",
    "# # print(grand_loss)\n",
    "# # grand_loss.backward()\n",
    "\n",
    "# # print(output.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x.grad)\n",
    "# # print(out.grad)\n",
    "# # print(WT.grad)\n",
    "# print((out.grad @ WT.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# graph = make_dot(grand_loss, params=parameters)\n",
    "\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the Forward Graph (used for conventional SGD)\n",
    "\n",
    "# graph = make_dot(output)\n",
    "# graph.render(\"attached\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(layers[0].weight)\n",
    "# print(parameters_val[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
