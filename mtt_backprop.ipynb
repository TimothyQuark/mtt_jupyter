{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to calculate backprop for the grand loss in MTT, i.e. 'gradient of gradients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we test using only linear layers\n",
    "\n",
    "# Linear layers y = xWT + b\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "in_dim = (5, 4)\n",
    "out_dim = 7\n",
    "x1 = torch.rand(*in_dim, requires_grad=True)\n",
    "WT1 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "b1 = torch.randn(out_dim, requires_grad=True)\n",
    "y1 = x1 @ WT1 + b1\n",
    "y1.retain_grad()\n",
    "# print(y1.shape)\n",
    "\n",
    "# Layer 2\n",
    "in_dim = y1.shape\n",
    "out_dim = 4\n",
    "WT2 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "b2 = torch.randn(out_dim, requires_grad=True)\n",
    "y2 = y1 @ WT2 + b2\n",
    "y2.retain_grad()\n",
    "# print(y2.shape)\n",
    "\n",
    "# Layer 3\n",
    "in_dim = y2.shape\n",
    "out_dim = 3\n",
    "WT3 = torch.rand(in_dim[-1], out_dim, requires_grad=True)\n",
    "b3 = torch.randn(out_dim, requires_grad=True)\n",
    "out = y2 @ WT3 + b3\n",
    "out.retain_grad() # Output of network\n",
    "\n",
    "# Sanity check\n",
    "# print(out == (((x1 @ WT1 + b1) @ WT2 + b2)) @ WT3 + b3)\n",
    "\n",
    "# Loss\n",
    "target = torch.randn(out.shape)\n",
    "loss_fn = nn.MSELoss()\n",
    "output = loss_fn(out, target)\n",
    "output.retain_grad()\n",
    "output.backward()\n",
    "\n",
    "dout_val = 2 * (out - target) / np.prod(out.shape) # derivative of MSELoss\n",
    "dx1_val = 0\n",
    "\n",
    "# print(dout_val)\n",
    "# print(out.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only tensors that are not computed from other tensors are leaves.\n",
    "# This tells us that all weight and bias tensors are leaves initally.\n",
    "# for var in [x1, y2, out, WT1, WT2, WT3, b1, b2, b3]:\n",
    "#     print(var.is_leaf)\n",
    "\n",
    "# print(WT1)\n",
    "# print(WT1.grad)\n",
    "\n",
    "# optimizer = torch.optim.SGD([WT1, WT2, WT3, b1, b2, b3], lr=0.1)\n",
    "# # Perform one step of SGD\n",
    "# optimizer.step()\n",
    "\n",
    "# # WT1 has now been updated, but is still a leaf (no dependencies!)\n",
    "# print(WT1)\n",
    "\n",
    "# p_stacked = torch.cat[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6664, grad_fn=<DivBackward0>)\n",
      "None\n",
      "tensor([ 0.0258,  0.0180, -0.0020, -0.0198,  0.0234,  0.0145,  0.0194,  0.0134,\n",
      "        -0.0003,  0.0318,  0.0197,  0.0276,  0.0084,  0.0098,  0.0188,  0.0068,\n",
      "         0.0016,  0.0029, -0.0031, -0.0044, -0.0030,  0.0082, -0.0004, -0.0117,\n",
      "         0.0126, -0.0087,  0.0401, -0.0018, -0.0074,  0.0162, -0.0184,  0.0083,\n",
      "         0.0078, -0.0076,  0.0018, -0.0013,  0.0152, -0.0048, -0.0216,  0.0157,\n",
      "        -0.0019,  0.0010, -0.0014,  0.0116,  0.0028,  0.0244,  0.0031,  0.0005,\n",
      "         0.0152,  0.0075,  0.0206, -0.0133, -0.0091, -0.0113, -0.0295,  0.0281,\n",
      "        -0.0168, -0.0091,  0.0024, -0.0074, -0.0117,  0.0164, -0.0103, -0.0116,\n",
      "         0.0151, -0.0035,  0.0132,  0.0209, -0.0029, -0.0088, -0.0212, -0.0243,\n",
      "         0.0004,  0.0065,  0.0111, -0.0009,  0.0128,  0.0265, -0.0011, -0.0164,\n",
      "         0.0159, -0.0255])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the grand loss as described in MTT\n",
    "\n",
    "# First flatten and concatenate all the parameters\n",
    "parameters = [WT1, WT2, WT3, b1, b2, b3]\n",
    "# All the parameters should be leaves because they don't have dependencies\n",
    "# for var in parameters:\n",
    "#     print(var.is_leaf)\n",
    "p_stacked = torch.cat([p.reshape(-1) for p in parameters])\n",
    "p_stacked.retain_grad()\n",
    "# Sanity check\n",
    "num_param = sum(np.prod(p.shape) for p in parameters)\n",
    "# print(p_stacked.shape[0] == num_param)\n",
    "\n",
    "# Imaginary start and end expert parameters\n",
    "exp_start_stacked = torch.randn(p_stacked.shape)\n",
    "exp_end_stacked = torch.randn(p_stacked.shape)\n",
    "\n",
    "param_loss_mse = nn.MSELoss(reduction='sum')\n",
    "param_dist_mse = nn.MSELoss(reduction='sum')\n",
    "\n",
    "param_loss = param_loss_mse(p_stacked, exp_end_stacked)\n",
    "param_loss /= num_param\n",
    "param_dist = param_dist_mse(exp_start_stacked, exp_end_stacked)\n",
    "param_dist /= num_param\n",
    "\n",
    "grand_loss = param_loss / param_dist\n",
    "\n",
    "print(grand_loss)\n",
    "\n",
    "print(p_stacked.grad)\n",
    "grand_loss.backward()\n",
    "# print(grand_loss)\n",
    "print(p_stacked.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
