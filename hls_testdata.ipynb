{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def save_var(path, name, npy_arrays: Dict[str, Any], it):\n",
    "    \"\"\"\n",
    "    Helper function to save Numpy arrays to use for testing HLS components.\n",
    "    path: location of folder where files are saved to\n",
    "    name: appended to file name of every Numpy file\n",
    "    npy_arrays: Dict of numpy arrays to save, key is used to name file, value is the Numpy array, not PyTorch tensor!.\n",
    "    it: iteration number, appended to end of file name\n",
    "    \"\"\"\n",
    "    for key, val in npy_arrays.items():\n",
    "        with open(path / f\"{name}_{key}_{it}.npy\", \"wb\") as f:\n",
    "            np.save(f, val)\n",
    "\n",
    "        print(f\"Saved variable {key} to {path}\")\n",
    "\n",
    "\n",
    "def pad_with(tensor, pad_width, iaxis, kwargs):\n",
    "    \"\"\" \"\n",
    "    Helper function to pad matrices and tensors. Note this pads all dimensions, not just the H and W channels which\n",
    "    are usually what we want in 2D convolution\n",
    "    See https://numpy.org/doc/stable/reference/generated/numpy.pad.html\n",
    "    \"\"\"\n",
    "    pad_value = kwargs.get(\"padder\", 10)\n",
    "    tensor[: pad_width[0]] = pad_value\n",
    "    tensor[-pad_width[1] :] = pad_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x gradients are approximately the same: True\n",
      "Saved variable x to ../../mtt_components/relu/tests\n",
      "Saved variable dx to ../../mtt_components/relu/tests\n",
      "Saved variable out to ../../mtt_components/relu/tests\n",
      "Saved variable dout to ../../mtt_components/relu/tests\n"
     ]
    }
   ],
   "source": [
    "# ReLU testdata generation\n",
    "\n",
    "\n",
    "# shape = (256, 128, 32, 32) to test HLS. Beware of stack overflow for big tensors, use std containers in C++!\n",
    "shape = (4, 3, 2, 2) # Shape: [Batch size, input data dimensions]\n",
    "\n",
    "x = torch.randn(*shape, dtype=torch.float, requires_grad=True)\n",
    "relu = nn.ReLU()\n",
    "out = relu(x)  # Forward pass\n",
    "\n",
    "target = torch.randn(*shape)  # Imaginary target values, defines shape of dout\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(out, target)  # Compute loss\n",
    "loss.backward()  # Backward pass\n",
    "\n",
    "# Validation\n",
    "dout = 2 * (out - target) / np.prod(shape)  # derivative of MSELoss\n",
    "cache = x.clone().detach()  # Must clone x so it isn't overwritten\n",
    "cache[cache <= 0] = 0\n",
    "dx = cache\n",
    "dx[dx > 0] = 1\n",
    "dx_truth = dout * dx\n",
    "print(\n",
    "    f\"x gradients are approximately the same: {torch.allclose(dx_truth, x.grad)}\"\n",
    ")\n",
    "\n",
    "# File name: shape_variable_testNum\n",
    "test_folder = Path(\"../../mtt_components/relu/tests\")\n",
    "name = f\"relu_{shape[0]}_{shape[1]}_{shape[2]}_{shape[3]}\"\n",
    "it = 1\n",
    "npy_arrays = {\n",
    "    \"x\": x.detach().numpy(),\n",
    "    \"dx\": x.grad.detach().numpy(),\n",
    "    \"out\": out.detach().numpy(),\n",
    "    \"dout\": dout.detach().numpy(),\n",
    "}\n",
    "\n",
    "save_var(test_folder, name, npy_arrays, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x gradients are approximately the same: True\n",
      "w gradients are approximately the same: True\n",
      "b gradients are approximately the same: True\n",
      "out values are approximately the same: True\n",
      "Saved variable x to ../../mtt_components/linear_forward/tests\n",
      "Saved variable dx to ../../mtt_components/linear_forward/tests\n",
      "Saved variable w to ../../mtt_components/linear_forward/tests\n",
      "Saved variable dw to ../../mtt_components/linear_forward/tests\n",
      "Saved variable b to ../../mtt_components/linear_forward/tests\n",
      "Saved variable db to ../../mtt_components/linear_forward/tests\n",
      "Saved variable out to ../../mtt_components/linear_forward/tests\n",
      "Saved variable dout to ../../mtt_components/linear_forward/tests\n"
     ]
    }
   ],
   "source": [
    "# Affine/Linear layer testdata generation (random tensors)\n",
    "# Remember to make dimensions divisible by block shape (usually 2^n)\n",
    "\n",
    "in_shape = (8, 3, 2, 2)  # Shape: [Batch size, input data dimensions]\n",
    "layer_shape = (\n",
    "    np.prod(in_shape[1:]),\n",
    "    4,\n",
    ")  # Shape: [flattened input, output channel]\n",
    "\n",
    "# Reshape to [batch, flattened input]\n",
    "x = torch.randn(*in_shape, requires_grad=True).reshape(\n",
    "    (in_shape[0], np.prod(in_shape[1:]))\n",
    ")\n",
    "x.retain_grad()  # Retain gradients for tensor views, i.e. reshape\n",
    "\n",
    "linear = nn.Linear(*layer_shape)\n",
    "out_truth = linear(\n",
    "    x\n",
    ")  # This output will also be our dout, since loss is just a sum for MSELoss\n",
    "\n",
    "target = torch.randn(in_shape[0], layer_shape[1])  # Imaginary target values\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(out_truth, target)  # Compute loss\n",
    "loss.backward()\n",
    "\n",
    "dout_val = (\n",
    "    2 * (out_truth - target) / np.prod((in_shape[0], layer_shape[1]))\n",
    ")  # derivative of MSELoss\n",
    "dx_val = dout_val @ linear.weight\n",
    "dw_val = dout_val.T @ x\n",
    "db_val = torch.sum(dout_val, axis=0)\n",
    "out_val = x @ linear.weight.T + linear.bias\n",
    "print(\n",
    "    f\"x gradients are approximately the same: {torch.allclose(dx_val, x.grad)}\"\n",
    ")\n",
    "print(\n",
    "    f\"w gradients are approximately the same: {torch.allclose(dw_val, linear.weight.grad)}\"\n",
    ")\n",
    "print(\n",
    "    f\"b gradients are approximately the same: {torch.allclose(db_val, linear.bias.grad)}\"\n",
    ")\n",
    "print(\n",
    "    f\"out values are approximately the same: {torch.allclose(out_val, out_truth)}\"\n",
    ")\n",
    "\n",
    "\n",
    "test_folder = Path(\"../../mtt_components/linear_forward//tests\")\n",
    "name = f\"in_{in_shape[0]}_{in_shape[1]}_{in_shape[2]}_{in_shape[3]}_out_{layer_shape[1]}\"\n",
    "it = 1\n",
    "npy_arrays = {\n",
    "    \"x\": x.detach().numpy(),\n",
    "    \"dx\": x.grad.detach().numpy(),\n",
    "    \"w\": linear.weight.detach().numpy(),\n",
    "    \"dw\": linear.weight.grad.detach().numpy(),\n",
    "    \"b\": linear.bias.detach().numpy(),\n",
    "    \"db\": linear.bias.grad.detach().numpy(),\n",
    "    \"out\": out.detach().numpy(),\n",
    "    \"dout\": dout_val.detach().numpy(),\n",
    "}\n",
    "\n",
    "save_var(test_folder, name, npy_arrays, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses are approximately the same: True\n",
      "out gradients are approximately the same: True\n",
      "Saved variable y_truth to ../../mtt_components/CE_forward/tests\n",
      "Saved variable y_out to ../../mtt_components/CE_forward/tests\n",
      "Saved variable loss to ../../mtt_components/CE_forward/tests\n",
      "Saved variable dout to ../../mtt_components/CE_forward/tests\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy loss function\n",
    "\n",
    "in_shape = (4, 10)  # Shape [N, C] (C=logits, preactivation)\n",
    "\n",
    "# PyTorch Ground Truth\n",
    "out = torch.randn(\n",
    "    in_shape, requires_grad=True\n",
    ")  # Imaginary output of model, logits\n",
    "target = torch.empty(in_shape[0], dtype=torch.long).random_(\n",
    "    10\n",
    ")  # Imaginary target values. Must be type torch.long for CE function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # Loss function, default reduction is 'mean'\n",
    "loss_truth = loss_fn(out, target)  # Compute loss\n",
    "loss_truth.backward()  # Compute gradient\n",
    "dout_truth = out.grad  # PyTorch dout\n",
    "\n",
    "# Validation\n",
    "y_out = out.detach().numpy()  # Logits are not whole numbers!\n",
    "y_truth = target.detach().numpy().astype(np.int64)  # Equivalent to torch.long\n",
    "\n",
    "# Forward pass\n",
    "# Alternative Karpathy version: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\n",
    "y_truth_one_hot = np.zeros_like(y_out, dtype=int)  # Same shape as model output\n",
    "y_truth_one_hot[np.arange(y_out.shape[0]), y_truth] = (\n",
    "    1  # Assign index of label as 1, others are 0\n",
    ")\n",
    "y_out_exp = np.exp(y_out - np.max(y_out, axis=1, keepdims=True))\n",
    "y_out_probs = y_out_exp / np.sum(\n",
    "    y_out_exp, axis=1, keepdims=True\n",
    ")  # Cached for backwards\n",
    "loss_val = -y_truth_one_hot * np.log(y_out_probs)\n",
    "loss_val = loss_val.sum(axis=1).mean()\n",
    "\n",
    "# Backwards pass\n",
    "dout_val = y_out_probs\n",
    "dout_val[np.arange(y_out.shape[0]), y_truth] -= 1\n",
    "dout_val /= y_out.shape[0]  # Hand calculated dout to validate\n",
    "\n",
    "print(\n",
    "    f\"Losses are approximately the same: {np.allclose(loss_val, loss_truth.detach().numpy())}\"\n",
    ")\n",
    "print(\n",
    "    f\"out gradients are approximately the same: {np.allclose(dout_val, dout_truth.detach().numpy())}\"\n",
    ")\n",
    "\n",
    "# File name: shape_variable_testNum\n",
    "test_folder = Path(\"../../mtt_components/CE_forward/tests\")\n",
    "name = f\"CE_in_{in_shape[0]}_{in_shape[1]}\"\n",
    "it = 1\n",
    "npy_arrays = {\n",
    "    \"y_truth\": y_truth.astype(np.float32),  # Convert to float for HLS\n",
    "    \"y_out\": x.grad.detach().numpy(),\n",
    "    \"loss\": loss_truth.detach().numpy(),\n",
    "    \"dout\": dout_truth.detach().numpy(),\n",
    "}\n",
    "\n",
    "save_var(test_folder, name, npy_arrays, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution outputs are similar True\n",
      "Gradient of weights are similar True\n",
      "Gradient of x are similar True\n",
      "Gradient of bias are similar True\n"
     ]
    }
   ],
   "source": [
    "# Convolution layer testdata generation\n",
    "# Scipy functions used because similar libraries exist in HLS\n",
    "\n",
    "# Parameters\n",
    "padding = 1\n",
    "stride = 1\n",
    "kernel = 3\n",
    "bias = True\n",
    "N, Cin, Hin, Win = (7, 3, 5, 5)  # Input dimensions (N, Cin, Hin, Win)\n",
    "Cout = 4  # Output dimensions (N, Cout, Hout, Wout)\n",
    "Hout = int(1 + (Hin + 2 * padding - kernel) / stride)\n",
    "Wout = int(1 + (Win + 2 * padding - kernel) / stride)\n",
    "\n",
    "# PyTorch Ground Truth\n",
    "x = torch.randn((N, Cin, Hin, Win), requires_grad=True)\n",
    "conv = nn.Conv2d(\n",
    "    Cin, Cout, kernel_size=kernel, stride=stride, padding=padding, bias=bias\n",
    ")\n",
    "out_truth = conv(x)\n",
    "out_truth.retain_grad()  # Needed to retain gradient during autograd\n",
    "\n",
    "# Validation\n",
    "\n",
    "# Calculate loss through linear layer, then use CE\n",
    "lin_out = 7  # Number of out channels, for testing\n",
    "flatten = nn.Flatten()\n",
    "flattened = flatten(out_truth)\n",
    "linear = nn.Linear(Cout * Hout * Wout, lin_out)\n",
    "lin_out = linear(flattened)\n",
    "target = torch.randn(lin_out.shape)  # Imaginary target values\n",
    "loss_fn = nn.CrossEntropyLoss()  # Loss function, default reduction is 'mean'\n",
    "loss_truth = loss_fn(lin_out, target)  # Compute loss\n",
    "loss_truth.backward()  # Compute gradient\n",
    "dout_truth = (\n",
    "    out_truth.grad.detach()\n",
    ")  # Incoming gradient into conv layer, not linear!\n",
    "\n",
    "# Useful numpy arrays\n",
    "# x shape [N, Cin, Hin, Win]\n",
    "# x padded shape [N, Cin, Hin + 2 * padding, Win + 2 * padding]\n",
    "# dw shape [Cout, Cin, kernel, kernel]\n",
    "# dout shape [N, Cout, Hout, Wout]\n",
    "# bias shape [Cout]\n",
    "dout_npy = dout_truth.detach().numpy()\n",
    "weights = conv.weight.detach().numpy()\n",
    "bias = conv.bias.detach().numpy()\n",
    "dw_truth = conv.weight.grad.detach().numpy()\n",
    "db_truth = conv.bias.grad.detach().numpy()\n",
    "dx_truth = x.grad.detach().numpy()\n",
    "x_padded = np.pad(x.detach().numpy(), padding, pad_with, padder=0)\n",
    "x_padded = x_padded[\n",
    "    padding:-padding, padding:-padding\n",
    "]  # Strip padding on N and C dimensions\n",
    "\n",
    "# Forward pass\n",
    "out_val = np.zeros((N, Cout, Hout, Wout))\n",
    "for n in range(N):\n",
    "    for cout in range(Cout):  # Iterate over out kernels\n",
    "        for cin in range(Cin):\n",
    "            out_val[n, cout] += signal.correlate2d(\n",
    "                x_padded[n, cin], weights[cout, cin], \"valid\"\n",
    "            )\n",
    "        out_val[n, cout] += bias[cout]\n",
    "\n",
    "out_truth_npy = out_truth.detach().numpy()  # Convert to Numpy array\n",
    "# Out values are sometimes very small, use a more relaxed relative tolerance (0.1% tolerance)\n",
    "print(\n",
    "    f\"Convolution outputs are similar {np.allclose(out_truth_npy, out_val, rtol=1e-3, atol=1e-5)}\"\n",
    ")\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# Calculate dw and dx\n",
    "dw_val = np.zeros(weights.shape)\n",
    "dx_val = np.zeros(dx_truth.shape)\n",
    "for n in range(N):\n",
    "    for cout in range(Cout):\n",
    "        for cin in range(Cin):\n",
    "            dw_val[cout, cin] += signal.correlate2d(\n",
    "                x_padded[n, cin], dout_npy[n, cout], \"valid\"\n",
    "            )\n",
    "            # Strip gradients calculated for x padding\n",
    "            dx_val[n, cin] += signal.convolve2d(\n",
    "                dout_npy[n, cout], weights[cout, cin], \"full\"\n",
    "            )[padding:-padding, padding:-padding]\n",
    "\n",
    "print(\n",
    "    f\"Gradient of weights are similar {np.allclose(dw_val, dw_truth, rtol=1e-3, atol=1e-5)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Gradient of x are similar {np.allclose(dx_val, dx_truth, rtol=1e-3, atol=1e-5)}\"\n",
    ")\n",
    "\n",
    "# Calculate db\n",
    "db_val = np.zeros(bias.shape)\n",
    "for c in range(Cout):\n",
    "    db_val[c] = np.sum(dout_npy[:, c, :, :])\n",
    "\n",
    "print(\n",
    "    f\"Gradient of bias are similar {np.allclose(db_val, db_truth, rtol=1e-3, atol=1e-5)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized output is same : True\n",
      "Gradient of x is same: True\n",
      "Gradient of weights is same: True\n",
      "Gradient of bias is same: True\n"
     ]
    }
   ],
   "source": [
    "# Instance Norm (GroupNorm with groups=num of channels) testdata generation\n",
    "# Remember behavior is different for training and evaluation!\n",
    "\n",
    "(N, C, H, W) = (6, 3, 4, 4)\n",
    "\n",
    "# PyTorch Ground Truth\n",
    "x = torch.randn((N, C, H, W), requires_grad=True)\n",
    "\n",
    "norm = nn.GroupNorm(C, C, affine=True) # Could use InstanceNorm layer, but MTT uses this approach\n",
    "out_truth = norm(x)\n",
    "out_truth.retain_grad()  # Needed to retain gradient during autograd\n",
    "\n",
    "# Validation\n",
    "\n",
    "# Calculate loss through linear layer, then use CE\n",
    "lin_out = 7  # Number of out channels, for testing\n",
    "flatten = nn.Flatten()\n",
    "flattened = flatten(out_truth)\n",
    "linear = nn.Linear(C * H * W, lin_out)\n",
    "lin_out = linear(flattened)\n",
    "target = torch.randn(lin_out.shape)  # Imaginary target values\n",
    "loss_fn = nn.CrossEntropyLoss()  # Loss function, default reduction is 'mean'\n",
    "loss_truth = loss_fn(lin_out, target)  # Compute loss\n",
    "loss_truth.backward()  # Compute gradient\n",
    "dout_truth = (\n",
    "    out_truth.grad.detach()\n",
    ")  # Incoming gradient into norm layer, not linear!\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "eps = 0.00001 # use same as PyTorch default\n",
    "gamma = torch.ones(C) # Number of features, i.e. RGB channels\n",
    "gamma = gamma.view(1, -1, 1, 1) # Allow broadcasting to dimensions of x (1, num_features, 1, 1)\n",
    "beta = torch.zeros(C) # Number of features, i.e. RGB channels\n",
    "beta = beta.view(1, -1, 1, 1)\n",
    "mean = x.mean(dim=(2,3), keepdim=True)\n",
    "var = x.var(dim=(2,3), keepdim=True, unbiased=False)\n",
    "x_normalized = (x - mean) / torch.sqrt(var + eps)\n",
    "x_normalized = x_normalized * gamma + beta\n",
    "\n",
    "print(f\"Normalized output is same : {torch.allclose(out_truth, x_normalized, rtol=1e-3)}\")\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "dgamma = (dout_truth * x_normalized).sum(dim=(0, 2, 3), keepdim=True) # Gradient of weights\n",
    "dbeta = dout_truth.sum(dim=(0, 2, 3), keepdim=True) # Gradient of bias\n",
    "dx_normalized = dout_truth * gamma.view(1, -1, 1, 1)\n",
    "inv_std = 1.0 / torch.sqrt(var + eps)\n",
    "dvar = -0.5 * (dx_normalized * (x - mean) * inv_std ** 3).sum(dim=(2, 3), keepdim=True)\n",
    "dmean = (-dx_normalized * inv_std).sum(dim=(2, 3), keepdim=True) + dvar * (-2.0 / (H * W) * (x - mean).sum(dim=(2, 3), keepdim=True))\n",
    "dx_val = dx_normalized * inv_std + dvar * 2.0 * (x - mean) / (H * W) + dmean / (H * W)\n",
    "\n",
    "print(f\"Gradient of x is same: {torch.allclose(dx_val, x.grad, rtol=1e-3)}\")\n",
    "print(f\"Gradient of weights is same: {torch.allclose(dgamma.view(-1), norm.weight.grad, rtol=1e-3)}\")\n",
    "print(f\"Gradient of bias is same: {torch.allclose(dbeta.view(-1), norm.bias.grad, rtol=1e-3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Average Pooling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
